{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-1-markdown",
   "metadata": {},
   "source": [
    "##  Cross-Encoder + LightGBM ì•™ìƒë¸” ëª¨ë¸ (ë°ì´í„° ëˆ„ìˆ˜ í•´ê²° ë²„ì „)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ K-ê²¹ êµì°¨ ê²€ì¦ì„ ì ìš©í•˜ì—¬ ë°ì´í„° ëˆ„ìˆ˜ ë¬¸ì œë¥¼ í•´ê²°í•œ ì•™ìƒë¸” ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "\n",
    "### **âœ… ìˆ˜ì •ëœ ë°©ë²•ë¡ **\n",
    "1.  **íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§**: ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìˆ˜ì¹˜ ë° ë²”ì£¼í˜• íŠ¹ì§•ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "2.  **Cross-Encoder êµì°¨ ê²€ì¦ ì˜ˆì¸¡ (Out-of-Fold Prediction)**: 5-ê²¹ êµì°¨ ê²€ì¦ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ëˆ„ìˆ˜ê°€ ì—†ëŠ” 'semantic score'(`ce_score`)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ê° ë°ì´í„° í¬ì¸íŠ¸ì˜ ì ìˆ˜ëŠ” í•´ë‹¹ ë°ì´í„°ê°€ í¬í•¨ë˜ì§€ ì•Šì€ í›ˆë ¨ ë°ì´í„°ë¡œ í•™ìŠµëœ ëª¨ë¸ì— ì˜í•´ ì˜ˆì¸¡ë©ë‹ˆë‹¤.\n",
    "3.  **LightGBM í›ˆë ¨**: êµì°¨ ê²€ì¦ìœ¼ë¡œ ìƒì„±ëœ 'semantic score'ì™€ ë‹¤ë¥¸ íŠ¹ì§•ë“¤ì„ ê²°í•©í•˜ì—¬ ìµœì¢… LGBM ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤.\n",
    "4.  **ìµœì¢… ëª¨ë¸ ì €ì¥**: ì¶”ë¡ ì— ì‚¬ìš©í•  ìµœì¢… Cross-Encoder(ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ)ì™€ LGBM ëª¨ë¸, ê·¸ë¦¬ê³  ì „ì²˜ë¦¬ê¸°ë“¤ì„ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sentence_transformers import CrossEncoder, InputExample\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from scipy.sparse import hstack\n",
    "import lightgbm as lgb\n",
    "\n",
    "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
    "print(f\"GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# ğŸ“Š ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì •ë³´ í™•ì¸\n",
    "# ==========================================================\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "print(f\"ğŸ” ë°ì´í„° í˜•íƒœ: {train_df.shape}\")\n",
    "print(f\"ğŸ¯ íƒ€ê²Ÿ ë¶„í¬: {train_df['rule_violation'].value_counts().to_dict()}\")\n",
    "print(\"\\nğŸ“‹ ë°ì´í„° ìƒ˜í”Œ:\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# ğŸ› ï¸ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ë° ì…ë ¥ ì¤€ë¹„ í•¨ìˆ˜ ì •ì˜ (ê¸°ì¡´ê³¼ ë™ì¼)\n",
    "# ==========================================================\n",
    "def count_urls(text):\n",
    "    return len(re.findall(r'https?://\\S+|www\\.\\S+', str(text)))\n",
    "\n",
    "def count_exclaims(text):\n",
    "    return str(text).count('!')\n",
    "\n",
    "def count_questions(text):\n",
    "    return str(text).count('?')\n",
    "\n",
    "def upper_ratio(text):\n",
    "    s = str(text)\n",
    "    letters = [c for c in s if c.isalpha()]\n",
    "    if not letters:\n",
    "        return 0.0\n",
    "    upp = sum(1 for c in letters if c.isupper())\n",
    "    return upp / len(letters)\n",
    "\n",
    "def repeat_char_max(text):\n",
    "    longest = 1\n",
    "    last = ''\n",
    "    cur = 0\n",
    "    for ch in str(text):\n",
    "        if ch == last:\n",
    "            cur += 1\n",
    "        else:\n",
    "            longest = max(longest, cur)\n",
    "            cur = 1\n",
    "            last = ch\n",
    "    longest = max(longest, cur)\n",
    "    return longest\n",
    "\n",
    "def jaccard_similarity(text1, text2):\n",
    "    set1 = set(str(text1).lower().split())\n",
    "    set2 = set(str(text2).lower().split())\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['body_len'] = df['body'].astype(str).str.len()\n",
    "    df['rule_len'] = df['rule'].astype(str).str.len()\n",
    "    df['body_words'] = df['body'].astype(str).str.split().str.len()\n",
    "    df['url_cnt'] = df['body'].apply(count_urls)\n",
    "    df['exc_cnt'] = df['body'].apply(count_exclaims)\n",
    "    df['q_cnt'] = df['body'].apply(count_questions)\n",
    "    df['upper_rt'] = df['body'].apply(upper_ratio)\n",
    "    df['rep_run'] = df['body'].apply(repeat_char_max)\n",
    "    df['rule_body_jaccard'] = [jaccard_similarity(rule, body) for rule, body in zip(df['rule'], df['body'])]\n",
    "    return df\n",
    "\n",
    "def prepare_cross_encoder_input(rule, body, positive_ex1=None, negative_ex1=None):\n",
    "    rule_text = str(rule).strip()\n",
    "    comment_text = str(body).strip()\n",
    "    examples_text = \"\"\n",
    "    if pd.notna(positive_ex1) and str(positive_ex1).strip():\n",
    "        examples_text += f\" [ê¸ì •ì˜ˆì‹œ] {str(positive_ex1).strip()}\"\n",
    "    if pd.notna(negative_ex1) and str(negative_ex1).strip():\n",
    "        examples_text += f\" [ë¶€ì •ì˜ˆì‹œ] {str(negative_ex1).strip()}\"\n",
    "    full_input = f\"[ê·œì¹™] {rule_text}{examples_text} [ëŒ“ê¸€] {comment_text}\"\n",
    "    return full_input\n",
    "\n",
    "# íŠ¹ì§• ìƒì„± ì‹¤í–‰\n",
    "print(\"ğŸ”§ ê¸°ë³¸ íŠ¹ì§• ìƒì„± ì¤‘...\")\n",
    "train_df_featured = create_features(train_df)\n",
    "print(\"âœ… ê¸°ë³¸ íŠ¹ì§• ìƒì„± ì™„ë£Œ!\")\n",
    "\n",
    "# Cross-Encoder ì…ë ¥ ë° ë ˆì´ë¸” ì¤€ë¹„\n",
    "ce_inputs = []\n",
    "for idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"CE ì…ë ¥ ë°ì´í„° ì¤€ë¹„\"):\n",
    "    ce_input = prepare_cross_encoder_input(\n",
    "        row['rule'], row['body'], row.get('positive_example_1'), row.get('negative_example_1')\n",
    "    )\n",
    "    ce_inputs.append(ce_input)\n",
    "\n",
    "ce_inputs = np.array(ce_inputs)\n",
    "labels = train_df['rule_violation'].values\n",
    "print(f\"âœ… {len(ce_inputs)}ê°œì˜ Cross-Encoder ì…ë ¥ ë° ë ˆì´ë¸” ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5-markdown",
   "metadata": {},
   "source": [
    "### ## ğŸŒŸ 1ë‹¨ê³„: Cross-Encoderë¡œ ëˆ„ìˆ˜ ì—†ëŠ” íŠ¹ì§•(ce_score) ìƒì„± (K-Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import CESoftmaxAccuracyEvaluator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# =========================================================\n",
    "# ğŸš€ 1ë‹¨ê³„: Cross-Encoder ìµœì í™” ì„¤ì •\n",
    "# =========================================================\n",
    "N_SPLITS = 5\n",
    "# 1. ëª¨ë¸ëª… ë³€ê²½\n",
    "MODEL_NAME = 'microsoft/deberta-v3-base' \n",
    "\n",
    "# 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE = 16 # VRAM ì¶©ë¶„í•˜ë©´ 32ë¡œ ë³€ê²½\n",
    "EPOCHS = 5      # Evaluatorê°€ ìµœì ì ì„ ì°¾ìœ¼ë¯€ë¡œ ë„‰ë„‰í•˜ê²Œ ì„¤ì •\n",
    "WEIGHT_DECAY = 0.01\n",
    "# =========================================================\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "oof_ce_scores = np.zeros(len(train_df))\n",
    "\n",
    "print(f\"ğŸš€ {N_SPLITS}-ê²¹ êµì°¨ ê²€ì¦ (ëª¨ë¸: {MODEL_NAME})\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(ce_inputs, labels)):\n",
    "    print(f'\\n===== Fold {fold+1}/{N_SPLITS} =====')\n",
    "    \n",
    "    X_train_fold, X_val_fold = ce_inputs[train_idx], ce_inputs[val_idx]\n",
    "    y_train_fold, y_val_fold = labels[train_idx], labels[val_idx]\n",
    "    \n",
    "    # ê° Foldì˜ í›ˆë ¨ ë°ì´í„° ì¤‘ 10%ë¥¼ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •ì„ ìœ„í•œ ê²€ì¦ì…‹ìœ¼ë¡œ ë¶„ë¦¬\n",
    "    train_texts, eval_texts, train_labels, eval_labels = train_test_split(\n",
    "        X_train_fold, y_train_fold, test_size=0.1, random_state=42, stratify=y_train_fold\n",
    "    )\n",
    "    \n",
    "    # í›ˆë ¨ìš© InputExample ìƒì„±\n",
    "    train_examples = []\n",
    "    for i in range(len(train_texts)):\n",
    "        text, label = train_texts[i], train_labels[i]\n",
    "        rule_part, comment_part = text.split('[ëŒ“ê¸€]', 1) if '[ëŒ“ê¸€]' in text else (text, \"\")\n",
    "        train_examples.append(InputExample(texts=[rule_part.strip(), comment_part.strip()], label=float(label)))\n",
    "\n",
    "    # ê²€ì¦ìš© InputExample ìƒì„±\n",
    "    eval_examples = []\n",
    "    for i in range(len(eval_texts)):\n",
    "        text, label = eval_texts[i], eval_labels[i]\n",
    "        rule_part, comment_part = text.split('[ëŒ“ê¸€]', 1) if '[ëŒ“ê¸€]' in text else (text, \"\")\n",
    "        eval_examples.append(InputExample(texts=[rule_part.strip(), comment_part.strip()], label=float(label)))\n",
    "\n",
    "    # ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    ce_model_fold = CrossEncoder(MODEL_NAME, num_labels=1, device=device)\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    warmup_steps = int(len(train_dataloader) * 0.1)\n",
    "\n",
    "    # ğŸ’¡ ì„±ëŠ¥ í‰ê°€ê¸°(Evaluator) ì„¤ì •\n",
    "    # í›ˆë ¨ ì¤‘ ì£¼ê¸°ì ìœ¼ë¡œ ê²€ì¦ì…‹ì˜ ì •í™•ë„ë¥¼ í‰ê°€í•˜ì—¬ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    evaluator = CESoftmaxAccuracyEvaluator.from_input_examples(eval_examples, name=f'fold-{fold}-eval')\n",
    "    \n",
    "    print(f\"Fold {fold+1} í›ˆë ¨ ì‹œì‘...\")\n",
    "    ce_model_fold.fit(\n",
    "        train_dataloader=train_dataloader,\n",
    "        epochs=EPOCHS,\n",
    "        warmup_steps=warmup_steps,\n",
    "        evaluator=evaluator,\n",
    "        evaluation_steps=int(len(train_dataloader) * 0.2), # 1 ì—í­ ë‹¹ 5ë²ˆ í‰ê°€\n",
    "        optimizer_params={'lr': LEARNING_RATE}, # 3. Learning Rate ì„¤ì •\n",
    "        weight_decay=WEIGHT_DECAY,             # 4. Weight Decay ì„¤ì •\n",
    "        output_path=f'./model_output/best_model_fold_{fold}', # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # OOF ì˜ˆì¸¡ ì‹œì—ëŠ” ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ ëª¨ë¸ì„ ë‹¤ì‹œ ë¶ˆëŸ¬ì™€ì„œ ì‚¬ìš©\n",
    "    best_model = CrossEncoder(f'./model_output/best_model_fold_{fold}')\n",
    "    \n",
    "    print(f\"Fold {fold+1} ì˜ˆì¸¡ ì¤‘...\")\n",
    "    val_predict_inputs = []\n",
    "    for text in X_val_fold:\n",
    "        rule_part, comment_part = text.split('[ëŒ“ê¸€]', 1) if '[ëŒ“ê¸€]' in text else (text, \"\")\n",
    "        val_predict_inputs.append([rule_part.strip(), comment_part.strip()])\n",
    "        \n",
    "    predictions = best_model.predict(val_predict_inputs, show_progress_bar=True)\n",
    "    oof_ce_scores[val_idx] = predictions\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë“  í´ë“œì— ëŒ€í•œ Out-of-Fold ì˜ˆì¸¡ ì™„ë£Œ!\")\n",
    "train_df_featured['ce_score'] = oof_ce_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7-markdown",
   "metadata": {},
   "source": [
    "### ## ğŸŒ³ 2ë‹¨ê³„: LightGBM ëª¨ë¸ í›ˆë ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ LGBM ëª¨ë¸ì„ ìœ„í•œ íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ ë° ì¸ì½”ë”© ì¤‘...\")\n",
    "\n",
    "# 1. ìˆ˜ì¹˜ íŠ¹ì§• (OOF ì˜ˆì¸¡ ì ìˆ˜ì¸ ce_score í¬í•¨)\n",
    "numerical_cols = [\n",
    "    'body_len', 'rule_len', 'body_words', 'url_cnt', 'exc_cnt', 'q_cnt',\n",
    "    'upper_rt', 'rep_run', 'rule_body_jaccard', \n",
    "    'ce_score' \n",
    "]\n",
    "scaler = StandardScaler()\n",
    "numerical_features = scaler.fit_transform(train_df_featured[numerical_cols])\n",
    "\n",
    "# 2. ë²”ì£¼í˜• íŠ¹ì§•\n",
    "categorical_cols = ['subreddit']\n",
    "onehot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "categorical_features = onehot_encoder.fit_transform(train_df_featured[categorical_cols])\n",
    "\n",
    "# 3. ëª¨ë“  íŠ¹ì§• ê²°í•©\n",
    "X_lgbm = hstack([numerical_features, categorical_features])\n",
    "y_lgbm = train_df_featured['rule_violation'].values\n",
    "\n",
    "print(f\"âœ… LGBM í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ. ìµœì¢… í˜•íƒœ: {X_lgbm.shape}\")\n",
    "\n",
    "# 4. LGBM ëª¨ë¸ í›ˆë ¨\n",
    "print(\"\\nğŸš€ LightGBM ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
    "lgbm_model = lgb.LGBMClassifier(\n",
    "    objective='binary', metric='auc', n_estimators=1000, learning_rate=0.05,\n",
    "    num_leaves=31, max_depth=-1, random_state=42, n_jobs=-1,\n",
    "    colsample_bytree=0.8, subsample=0.8\n",
    ")\n",
    "\n",
    "lgbm_model.fit(X_lgbm, y_lgbm, \n",
    "             eval_set=[(X_lgbm, y_lgbm)],\n",
    "             eval_metric='auc',\n",
    "             callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "\n",
    "print(\"âœ… LightGBM ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9-markdown",
   "metadata": {},
   "source": [
    "### ## ğŸ’¾ 3ë‹¨ê³„: ìµœì¢… ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶”ë¡  ì‹œ test ë°ì´í„°ì— ì ìš©í•˜ê¸° ìœ„í•´ Cross-Encoderë¥¼ ì „ì²´ ë°ì´í„°ë¡œ ë‹¤ì‹œ í•™ìŠµ\n",
    "print(\"ğŸš€ ì¶”ë¡ ìš© ìµœì¢… Cross-Encoder ëª¨ë¸ì„ ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµí•©ë‹ˆë‹¤...\")\n",
    "\n",
    "final_cross_encoder = CrossEncoder(MODEL_NAME, num_labels=1, device=device)\n",
    "\n",
    "# ì „ì²´ í›ˆë ¨ ë°ì´í„° ì¤€ë¹„\n",
    "final_train_examples = []\n",
    "for i in range(len(ce_inputs)):\n",
    "    text = ce_inputs[i]\n",
    "    rule_part, comment_part = text.split('[ëŒ“ê¸€]', 1) if '[ëŒ“ê¸€]' in text else (text, \"\")\n",
    "    final_train_examples.append(InputExample(texts=[rule_part.strip(), comment_part.strip()], label=float(labels[i])))\n",
    "\n",
    "final_train_dataloader = DataLoader(final_train_examples, shuffle=True, batch_size=16)\n",
    "warmup_steps = int(len(final_train_dataloader) * 0.1)\n",
    "\n",
    "final_cross_encoder.fit(\n",
    "    train_dataloader=final_train_dataloader,\n",
    "    epochs=1, # ì „ì²´ ë°ì´í„°ì´ë¯€ë¡œ 1 ì—í­ìœ¼ë¡œ ì¶©ë¶„\n",
    "    warmup_steps=warmup_steps,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "print(\"âœ… ìµœì¢… Cross-Encoder ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "\n",
    "# ==========================================================\n",
    "# ğŸ’¾ ìµœì¢… ëª¨ë¸ ë° ì „ì²˜ë¦¬ ê°ì²´ ì €ì¥\n",
    "# ==========================================================\n",
    "output_dir = './model_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Cross-Encoder ëª¨ë¸ ì €ì¥\n",
    "ce_output_path = os.path.join(output_dir, 'final_cross_encoder_model')\n",
    "final_cross_encoder.save(ce_output_path)\n",
    "print(f\"âœ… Cross-Encoder ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {ce_output_path}\")\n",
    "\n",
    "# 2. LGBM ëª¨ë¸ ì €ì¥\n",
    "joblib.dump(lgbm_model, os.path.join(output_dir, 'lgbm_model.pkl'))\n",
    "print(f\"âœ… LGBM ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {os.path.join(output_dir, 'lgbm_model.pkl')}\")\n",
    "\n",
    "# 3. Scaler ì €ì¥\n",
    "joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))\n",
    "print(f\"âœ… Scaler ì €ì¥ ì™„ë£Œ: {os.path.join(output_dir, 'scaler.pkl')}\")\n",
    "\n",
    "# 4. OneHotEncoder ì €ì¥\n",
    "joblib.dump(onehot_encoder, os.path.join(output_dir, 'onehot_encoder.pkl'))\n",
    "print(f\"âœ… OneHotEncoder ì €ì¥ ì™„ë£Œ: {os.path.join(output_dir, 'onehot_encoder.pkl')}\")\n",
    "\n",
    "# 5. ìˆ˜ì¹˜ íŠ¹ì§• ì»¬ëŸ¼ëª… ì €ì¥\n",
    "with open(os.path.join(output_dir, 'numerical_cols.pkl'), 'wb') as f:\n",
    "    pickle.dump(numerical_cols, f)\n",
    "print(f\"âœ… ìˆ˜ì¹˜ íŠ¹ì§• ì»¬ëŸ¼ëª… ì €ì¥ ì™„ë£Œ: {os.path.join(output_dir, 'numerical_cols.pkl')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
