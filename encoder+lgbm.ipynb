{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-1-markdown",
   "metadata": {},
   "source": [
    "##  Cross-Encoder + LightGBM 앙상블 모델 (데이터 누수 해결 버전)\n",
    "\n",
    "이 노트북은 K-겹 교차 검증을 적용하여 데이터 누수 문제를 해결한 앙상블 모델을 구축합니다.\n",
    "\n",
    "### **✅ 수정된 방법론**\n",
    "1.  **특징 엔지니어링**: 기존과 동일하게 수치 및 범주형 특징을 생성합니다.\n",
    "2.  **Cross-Encoder 교차 검증 예측 (Out-of-Fold Prediction)**: 5-겹 교차 검증을 사용하여 데이터 누수가 없는 'semantic score'(`ce_score`)를 생성합니다. 각 데이터 포인트의 점수는 해당 데이터가 포함되지 않은 훈련 데이터로 학습된 모델에 의해 예측됩니다.\n",
    "3.  **LightGBM 훈련**: 교차 검증으로 생성된 'semantic score'와 다른 특징들을 결합하여 최종 LGBM 모델을 훈련합니다.\n",
    "4.  **최종 모델 저장**: 추론에 사용할 최종 Cross-Encoder(전체 데이터로 학습)와 LGBM 모델, 그리고 전처리기들을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sentence_transformers import CrossEncoder, InputExample\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from scipy.sparse import hstack\n",
    "import lightgbm as lgb\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "print(f\"GPU 사용 가능: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 디바이스: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 📊 데이터 로드 및 기본 정보 확인\n",
    "# ==========================================================\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "print(f\"🔍 데이터 형태: {train_df.shape}\")\n",
    "print(f\"🎯 타겟 분포: {train_df['rule_violation'].value_counts().to_dict()}\")\n",
    "print(\"\\n📋 데이터 샘플:\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 🛠️ 특징 엔지니어링 및 입력 준비 함수 정의 (기존과 동일)\n",
    "# ==========================================================\n",
    "def count_urls(text):\n",
    "    return len(re.findall(r'https?://\\S+|www\\.\\S+', str(text)))\n",
    "\n",
    "def count_exclaims(text):\n",
    "    return str(text).count('!')\n",
    "\n",
    "def count_questions(text):\n",
    "    return str(text).count('?')\n",
    "\n",
    "def upper_ratio(text):\n",
    "    s = str(text)\n",
    "    letters = [c for c in s if c.isalpha()]\n",
    "    if not letters:\n",
    "        return 0.0\n",
    "    upp = sum(1 for c in letters if c.isupper())\n",
    "    return upp / len(letters)\n",
    "\n",
    "def repeat_char_max(text):\n",
    "    longest = 1\n",
    "    last = ''\n",
    "    cur = 0\n",
    "    for ch in str(text):\n",
    "        if ch == last:\n",
    "            cur += 1\n",
    "        else:\n",
    "            longest = max(longest, cur)\n",
    "            cur = 1\n",
    "            last = ch\n",
    "    longest = max(longest, cur)\n",
    "    return longest\n",
    "\n",
    "def jaccard_similarity(text1, text2):\n",
    "    set1 = set(str(text1).lower().split())\n",
    "    set2 = set(str(text2).lower().split())\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['body_len'] = df['body'].astype(str).str.len()\n",
    "    df['rule_len'] = df['rule'].astype(str).str.len()\n",
    "    df['body_words'] = df['body'].astype(str).str.split().str.len()\n",
    "    df['url_cnt'] = df['body'].apply(count_urls)\n",
    "    df['exc_cnt'] = df['body'].apply(count_exclaims)\n",
    "    df['q_cnt'] = df['body'].apply(count_questions)\n",
    "    df['upper_rt'] = df['body'].apply(upper_ratio)\n",
    "    df['rep_run'] = df['body'].apply(repeat_char_max)\n",
    "    df['rule_body_jaccard'] = [jaccard_similarity(rule, body) for rule, body in zip(df['rule'], df['body'])]\n",
    "    return df\n",
    "\n",
    "def prepare_cross_encoder_input(rule, body, positive_ex1=None, negative_ex1=None):\n",
    "    rule_text = str(rule).strip()\n",
    "    comment_text = str(body).strip()\n",
    "    examples_text = \"\"\n",
    "    if pd.notna(positive_ex1) and str(positive_ex1).strip():\n",
    "        examples_text += f\" [긍정예시] {str(positive_ex1).strip()}\"\n",
    "    if pd.notna(negative_ex1) and str(negative_ex1).strip():\n",
    "        examples_text += f\" [부정예시] {str(negative_ex1).strip()}\"\n",
    "    full_input = f\"[규칙] {rule_text}{examples_text} [댓글] {comment_text}\"\n",
    "    return full_input\n",
    "\n",
    "# 특징 생성 실행\n",
    "print(\"🔧 기본 특징 생성 중...\")\n",
    "train_df_featured = create_features(train_df)\n",
    "print(\"✅ 기본 특징 생성 완료!\")\n",
    "\n",
    "# Cross-Encoder 입력 및 레이블 준비\n",
    "ce_inputs = []\n",
    "for idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"CE 입력 데이터 준비\"):\n",
    "    ce_input = prepare_cross_encoder_input(\n",
    "        row['rule'], row['body'], row.get('positive_example_1'), row.get('negative_example_1')\n",
    "    )\n",
    "    ce_inputs.append(ce_input)\n",
    "\n",
    "ce_inputs = np.array(ce_inputs)\n",
    "labels = train_df['rule_violation'].values\n",
    "print(f\"✅ {len(ce_inputs)}개의 Cross-Encoder 입력 및 레이블 준비 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5-markdown",
   "metadata": {},
   "source": [
    "### ## 🌟 1단계: Cross-Encoder로 누수 없는 특징(ce_score) 생성 (K-Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import CESoftmaxAccuracyEvaluator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# =========================================================\n",
    "# 🚀 1단계: Cross-Encoder 최적화 설정\n",
    "# =========================================================\n",
    "N_SPLITS = 5\n",
    "# 1. 모델명 변경\n",
    "MODEL_NAME = 'microsoft/deberta-v3-base' \n",
    "\n",
    "# 2. 하이퍼파라미터 설정\n",
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE = 16 # VRAM 충분하면 32로 변경\n",
    "EPOCHS = 5      # Evaluator가 최적점을 찾으므로 넉넉하게 설정\n",
    "WEIGHT_DECAY = 0.01\n",
    "# =========================================================\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "oof_ce_scores = np.zeros(len(train_df))\n",
    "\n",
    "print(f\"🚀 {N_SPLITS}-겹 교차 검증 (모델: {MODEL_NAME})\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(ce_inputs, labels)):\n",
    "    print(f'\\n===== Fold {fold+1}/{N_SPLITS} =====')\n",
    "    \n",
    "    X_train_fold, X_val_fold = ce_inputs[train_idx], ce_inputs[val_idx]\n",
    "    y_train_fold, y_val_fold = labels[train_idx], labels[val_idx]\n",
    "    \n",
    "    # 각 Fold의 훈련 데이터 중 10%를 모델 성능 측정을 위한 검증셋으로 분리\n",
    "    train_texts, eval_texts, train_labels, eval_labels = train_test_split(\n",
    "        X_train_fold, y_train_fold, test_size=0.1, random_state=42, stratify=y_train_fold\n",
    "    )\n",
    "    \n",
    "    # 훈련용 InputExample 생성\n",
    "    train_examples = []\n",
    "    for i in range(len(train_texts)):\n",
    "        text, label = train_texts[i], train_labels[i]\n",
    "        rule_part, comment_part = text.split('[댓글]', 1) if '[댓글]' in text else (text, \"\")\n",
    "        train_examples.append(InputExample(texts=[rule_part.strip(), comment_part.strip()], label=float(label)))\n",
    "\n",
    "    # 검증용 InputExample 생성\n",
    "    eval_examples = []\n",
    "    for i in range(len(eval_texts)):\n",
    "        text, label = eval_texts[i], eval_labels[i]\n",
    "        rule_part, comment_part = text.split('[댓글]', 1) if '[댓글]' in text else (text, \"\")\n",
    "        eval_examples.append(InputExample(texts=[rule_part.strip(), comment_part.strip()], label=float(label)))\n",
    "\n",
    "    # 모델 초기화\n",
    "    ce_model_fold = CrossEncoder(MODEL_NAME, num_labels=1, device=device)\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    warmup_steps = int(len(train_dataloader) * 0.1)\n",
    "\n",
    "    # 💡 성능 평가기(Evaluator) 설정\n",
    "    # 훈련 중 주기적으로 검증셋의 정확도를 평가하여 최고 성능 모델을 저장합니다.\n",
    "    evaluator = CESoftmaxAccuracyEvaluator.from_input_examples(eval_examples, name=f'fold-{fold}-eval')\n",
    "    \n",
    "    print(f\"Fold {fold+1} 훈련 시작...\")\n",
    "    ce_model_fold.fit(\n",
    "        train_dataloader=train_dataloader,\n",
    "        epochs=EPOCHS,\n",
    "        warmup_steps=warmup_steps,\n",
    "        evaluator=evaluator,\n",
    "        evaluation_steps=int(len(train_dataloader) * 0.2), # 1 에폭 당 5번 평가\n",
    "        optimizer_params={'lr': LEARNING_RATE}, # 3. Learning Rate 설정\n",
    "        weight_decay=WEIGHT_DECAY,             # 4. Weight Decay 설정\n",
    "        output_path=f'./model_output/best_model_fold_{fold}', # 최고 성능 모델 저장\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # OOF 예측 시에는 가장 성능이 좋았던 모델을 다시 불러와서 사용\n",
    "    best_model = CrossEncoder(f'./model_output/best_model_fold_{fold}')\n",
    "    \n",
    "    print(f\"Fold {fold+1} 예측 중...\")\n",
    "    val_predict_inputs = []\n",
    "    for text in X_val_fold:\n",
    "        rule_part, comment_part = text.split('[댓글]', 1) if '[댓글]' in text else (text, \"\")\n",
    "        val_predict_inputs.append([rule_part.strip(), comment_part.strip()])\n",
    "        \n",
    "    predictions = best_model.predict(val_predict_inputs, show_progress_bar=True)\n",
    "    oof_ce_scores[val_idx] = predictions\n",
    "\n",
    "print(\"\\n✅ 모든 폴드에 대한 Out-of-Fold 예측 완료!\")\n",
    "train_df_featured['ce_score'] = oof_ce_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7-markdown",
   "metadata": {},
   "source": [
    "### ## 🌳 2단계: LightGBM 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔧 LGBM 모델을 위한 특징 스케일링 및 인코딩 중...\")\n",
    "\n",
    "# 1. 수치 특징 (OOF 예측 점수인 ce_score 포함)\n",
    "numerical_cols = [\n",
    "    'body_len', 'rule_len', 'body_words', 'url_cnt', 'exc_cnt', 'q_cnt',\n",
    "    'upper_rt', 'rep_run', 'rule_body_jaccard', \n",
    "    'ce_score' \n",
    "]\n",
    "scaler = StandardScaler()\n",
    "numerical_features = scaler.fit_transform(train_df_featured[numerical_cols])\n",
    "\n",
    "# 2. 범주형 특징\n",
    "categorical_cols = ['subreddit']\n",
    "onehot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "categorical_features = onehot_encoder.fit_transform(train_df_featured[categorical_cols])\n",
    "\n",
    "# 3. 모든 특징 결합\n",
    "X_lgbm = hstack([numerical_features, categorical_features])\n",
    "y_lgbm = train_df_featured['rule_violation'].values\n",
    "\n",
    "print(f\"✅ LGBM 훈련 데이터 준비 완료. 최종 형태: {X_lgbm.shape}\")\n",
    "\n",
    "# 4. LGBM 모델 훈련\n",
    "print(\"\\n🚀 LightGBM 모델 훈련 시작...\")\n",
    "lgbm_model = lgb.LGBMClassifier(\n",
    "    objective='binary', metric='auc', n_estimators=1000, learning_rate=0.05,\n",
    "    num_leaves=31, max_depth=-1, random_state=42, n_jobs=-1,\n",
    "    colsample_bytree=0.8, subsample=0.8\n",
    ")\n",
    "\n",
    "lgbm_model.fit(X_lgbm, y_lgbm, \n",
    "             eval_set=[(X_lgbm, y_lgbm)],\n",
    "             eval_metric='auc',\n",
    "             callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "\n",
    "print(\"✅ LightGBM 모델 훈련 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9-markdown",
   "metadata": {},
   "source": [
    "### ## 💾 3단계: 최종 모델 및 전처리기 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 시 test 데이터에 적용하기 위해 Cross-Encoder를 전체 데이터로 다시 학습\n",
    "print(\"🚀 추론용 최종 Cross-Encoder 모델을 전체 데이터로 학습합니다...\")\n",
    "\n",
    "final_cross_encoder = CrossEncoder(MODEL_NAME, num_labels=1, device=device)\n",
    "\n",
    "# 전체 훈련 데이터 준비\n",
    "final_train_examples = []\n",
    "for i in range(len(ce_inputs)):\n",
    "    text = ce_inputs[i]\n",
    "    rule_part, comment_part = text.split('[댓글]', 1) if '[댓글]' in text else (text, \"\")\n",
    "    final_train_examples.append(InputExample(texts=[rule_part.strip(), comment_part.strip()], label=float(labels[i])))\n",
    "\n",
    "final_train_dataloader = DataLoader(final_train_examples, shuffle=True, batch_size=16)\n",
    "warmup_steps = int(len(final_train_dataloader) * 0.1)\n",
    "\n",
    "final_cross_encoder.fit(\n",
    "    train_dataloader=final_train_dataloader,\n",
    "    epochs=1, # 전체 데이터이므로 1 에폭으로 충분\n",
    "    warmup_steps=warmup_steps,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "print(\"✅ 최종 Cross-Encoder 모델 학습 완료!\")\n",
    "\n",
    "# ==========================================================\n",
    "# 💾 최종 모델 및 전처리 객체 저장\n",
    "# ==========================================================\n",
    "output_dir = './model_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Cross-Encoder 모델 저장\n",
    "ce_output_path = os.path.join(output_dir, 'final_cross_encoder_model')\n",
    "final_cross_encoder.save(ce_output_path)\n",
    "print(f\"✅ Cross-Encoder 모델 저장 완료: {ce_output_path}\")\n",
    "\n",
    "# 2. LGBM 모델 저장\n",
    "joblib.dump(lgbm_model, os.path.join(output_dir, 'lgbm_model.pkl'))\n",
    "print(f\"✅ LGBM 모델 저장 완료: {os.path.join(output_dir, 'lgbm_model.pkl')}\")\n",
    "\n",
    "# 3. Scaler 저장\n",
    "joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))\n",
    "print(f\"✅ Scaler 저장 완료: {os.path.join(output_dir, 'scaler.pkl')}\")\n",
    "\n",
    "# 4. OneHotEncoder 저장\n",
    "joblib.dump(onehot_encoder, os.path.join(output_dir, 'onehot_encoder.pkl'))\n",
    "print(f\"✅ OneHotEncoder 저장 완료: {os.path.join(output_dir, 'onehot_encoder.pkl')}\")\n",
    "\n",
    "# 5. 수치 특징 컬럼명 저장\n",
    "with open(os.path.join(output_dir, 'numerical_cols.pkl'), 'wb') as f:\n",
    "    pickle.dump(numerical_cols, f)\n",
    "print(f\"✅ 수치 특징 컬럼명 저장 완료: {os.path.join(output_dir, 'numerical_cols.pkl')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
