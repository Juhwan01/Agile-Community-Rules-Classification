{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df622495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cache_dir = \"C:/huggingface_cache\"\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir # ë§Œì•½ì„ ìœ„í•´ ì´ì¤‘ìœ¼ë¡œ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1781db68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU ì‚¬ìš© ê°€ëŠ¥: True\n",
      "ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“š í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Kaggle ì…ë ¥ ë°ì´í„° í™•ì¸\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c83e6f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ë°ì´í„° í˜•íƒœ: (2029, 9)\n",
      "ğŸ¯ íƒ€ê²Ÿ ë¶„í¬: {1: 1031, 0: 998}\n",
      "\n",
      "ğŸ“‹ ë°ì´í„° ìƒ˜í”Œ:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>body</th>\n",
       "      <th>rule</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>positive_example_1</th>\n",
       "      <th>positive_example_2</th>\n",
       "      <th>negative_example_1</th>\n",
       "      <th>negative_example_2</th>\n",
       "      <th>rule_violation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Banks don't want you to know this! Click here ...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>Futurology</td>\n",
       "      <td>If you could tell your younger self something ...</td>\n",
       "      <td>hunt for lady for jack off in neighbourhood ht...</td>\n",
       "      <td>Watch Golden Globe Awards 2017 Live Online in ...</td>\n",
       "      <td>DOUBLE CEE x BANDS EPPS - \"BIRDS\"\\n\\nDOWNLOAD/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>SD Stream [ ENG Link 1] (http://www.sportsstre...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>soccerstreams</td>\n",
       "      <td>[I wanna kiss you all over! Stunning!](http://...</td>\n",
       "      <td>LOLGA.COM is One of the First Professional Onl...</td>\n",
       "      <td>#Rapper \\nğŸš¨Straight Outta Cross Keys SC ğŸš¨YouTu...</td>\n",
       "      <td>[15 Amazing Hidden Features Of Google Search Y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Lol. Try appealing the ban and say you won't d...</td>\n",
       "      <td>No legal advice: Do not offer or request legal...</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>Don't break up with him or call the cops.  If ...</td>\n",
       "      <td>It'll be dismissed: https://en.wikipedia.org/w...</td>\n",
       "      <td>Where is there a site that still works where y...</td>\n",
       "      <td>Because this statement of his is true. It isn'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>she will come your home open her legs with  an...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>sex</td>\n",
       "      <td>Selling Tyrande codes for 3â‚¬ to paypal. PM. \\n...</td>\n",
       "      <td>tight pussy watch for your cock get her at thi...</td>\n",
       "      <td>NSFW(obviously) http://spankbang.com/iy3u/vide...</td>\n",
       "      <td>Good News ::Download WhatsApp 2.16.230 APK for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>code free tyrande ---&gt;&gt;&gt; [Imgur](http://i.imgu...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>hearthstone</td>\n",
       "      <td>wow!! amazing reminds me of the old days.Well...</td>\n",
       "      <td>seek for lady for sex in around http://p77.pl/...</td>\n",
       "      <td>must be watch movie https://sites.google.com/s...</td>\n",
       "      <td>We're streaming Pokemon Veitnamese Crystal RIG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id                                               body  \\\n",
       "0       0  Banks don't want you to know this! Click here ...   \n",
       "1       1  SD Stream [ ENG Link 1] (http://www.sportsstre...   \n",
       "2       2  Lol. Try appealing the ban and say you won't d...   \n",
       "3       3  she will come your home open her legs with  an...   \n",
       "4       4  code free tyrande --->>> [Imgur](http://i.imgu...   \n",
       "\n",
       "                                                rule      subreddit  \\\n",
       "0  No Advertising: Spam, referral links, unsolici...     Futurology   \n",
       "1  No Advertising: Spam, referral links, unsolici...  soccerstreams   \n",
       "2  No legal advice: Do not offer or request legal...   pcmasterrace   \n",
       "3  No Advertising: Spam, referral links, unsolici...            sex   \n",
       "4  No Advertising: Spam, referral links, unsolici...    hearthstone   \n",
       "\n",
       "                                  positive_example_1  \\\n",
       "0  If you could tell your younger self something ...   \n",
       "1  [I wanna kiss you all over! Stunning!](http://...   \n",
       "2  Don't break up with him or call the cops.  If ...   \n",
       "3  Selling Tyrande codes for 3â‚¬ to paypal. PM. \\n...   \n",
       "4   wow!! amazing reminds me of the old days.Well...   \n",
       "\n",
       "                                  positive_example_2  \\\n",
       "0  hunt for lady for jack off in neighbourhood ht...   \n",
       "1  LOLGA.COM is One of the First Professional Onl...   \n",
       "2  It'll be dismissed: https://en.wikipedia.org/w...   \n",
       "3  tight pussy watch for your cock get her at thi...   \n",
       "4  seek for lady for sex in around http://p77.pl/...   \n",
       "\n",
       "                                  negative_example_1  \\\n",
       "0  Watch Golden Globe Awards 2017 Live Online in ...   \n",
       "1  #Rapper \\nğŸš¨Straight Outta Cross Keys SC ğŸš¨YouTu...   \n",
       "2  Where is there a site that still works where y...   \n",
       "3  NSFW(obviously) http://spankbang.com/iy3u/vide...   \n",
       "4  must be watch movie https://sites.google.com/s...   \n",
       "\n",
       "                                  negative_example_2  rule_violation  \n",
       "0  DOUBLE CEE x BANDS EPPS - \"BIRDS\"\\n\\nDOWNLOAD/...               0  \n",
       "1  [15 Amazing Hidden Features Of Google Search Y...               0  \n",
       "2  Because this statement of his is true. It isn'...               1  \n",
       "3  Good News ::Download WhatsApp 2.16.230 APK for...               1  \n",
       "4  We're streaming Pokemon Veitnamese Crystal RIG...               1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# ğŸ“Š ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì •ë³´ í™•ì¸\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "print(f\"ğŸ” ë°ì´í„° í˜•íƒœ: {train_df.shape}\")\n",
    "print(f\"ğŸ¯ íƒ€ê²Ÿ ë¶„í¬: {train_df['rule_violation'].value_counts().to_dict()}\")\n",
    "print(\"\\nğŸ“‹ ë°ì´í„° ìƒ˜í”Œ:\")\n",
    "display(train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a66819f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...\n",
      "ğŸ“ ê¸°ë³¸ í…ìŠ¤íŠ¸ íŠ¹ì§• ìƒì„± ì¤‘...\n",
      "ğŸ¨ ìŠ¤íƒ€ì¼ íŠ¹ì§• ìƒì„± ì¤‘...\n",
      "ğŸ˜Š ì´ëª¨ì§€ ë° íŠ¹ìˆ˜ë¬¸ì íŠ¹ì§• ìƒì„± ì¤‘...\n",
      "ğŸ“š ì–´íœ˜ ê¸°ë°˜ íŠ¹ì§• ìƒì„± ì¤‘...\n",
      "ğŸ”— ê·œì¹™-ëŒ“ê¸€ ìƒí˜¸ì‘ìš© íŠ¹ì§• ìƒì„± ì¤‘...\n",
      "âœ… íŠ¹ì§• ìƒì„± ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# ğŸ› ï¸ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ í•¨ìˆ˜ ì •ì˜\n",
    "def count_urls(text):\n",
    "    \"\"\"URL ê°œìˆ˜ ì„¸ê¸°\"\"\"\n",
    "    return len(re.findall(r'https?://\\S+|www\\.\\S+', str(text)))\n",
    "\n",
    "def count_exclaims(text):\n",
    "    \"\"\"ê°íƒ„í‘œ ê°œìˆ˜ ì„¸ê¸°\"\"\"\n",
    "    return str(text).count('!')\n",
    "\n",
    "def count_questions(text):\n",
    "    \"\"\"ë¬¼ìŒí‘œ ê°œìˆ˜ ì„¸ê¸°\"\"\"\n",
    "    return str(text).count('?')\n",
    "\n",
    "def upper_ratio(text):\n",
    "    \"\"\"ëŒ€ë¬¸ì ë¹„ìœ¨ ê³„ì‚°\"\"\"\n",
    "    s = str(text)\n",
    "    letters = [c for c in s if c.isalpha()]\n",
    "    if not letters:\n",
    "        return 0.0\n",
    "    upp = sum(1 for c in letters if c.isupper())\n",
    "    return upp / len(letters)\n",
    "\n",
    "def repeat_char_max(text):\n",
    "    \"\"\"ì—°ì†ëœ ë¬¸ìì˜ ìµœëŒ€ ê¸¸ì´\"\"\"\n",
    "    longest = 1\n",
    "    last = ''\n",
    "    cur = 0\n",
    "    for ch in str(text):\n",
    "        if ch == last:\n",
    "            cur += 1\n",
    "        else:\n",
    "            longest = max(longest, cur)\n",
    "            cur = 1\n",
    "            last = ch\n",
    "    longest = max(longest, cur)\n",
    "    return longest\n",
    "\n",
    "def jaccard_similarity(text1, text2):\n",
    "    \"\"\"ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚°\"\"\"\n",
    "    set1 = set(str(text1).lower().split())\n",
    "    set2 = set(str(text2).lower().split())\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "# ==========================================================\n",
    "# ğŸ”§ íŠ¹ì§• ìƒì„± í•¨ìˆ˜\n",
    "def create_features(df):\n",
    "    \"\"\"EDAì—ì„œ ë°œê²¬í•œ ìœ ìš©í•œ íŠ¹ì§•ë“¤ì„ ìƒì„±\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"ğŸ“ ê¸°ë³¸ í…ìŠ¤íŠ¸ íŠ¹ì§• ìƒì„± ì¤‘...\")\n",
    "    df['body_len'] = df['body'].astype(str).str.len()\n",
    "    df['rule_len'] = df['rule'].astype(str).str.len()\n",
    "    df['body_words'] = df['body'].astype(str).str.split().str.len()\n",
    "    \n",
    "    print(\"ğŸ¨ ìŠ¤íƒ€ì¼ íŠ¹ì§• ìƒì„± ì¤‘...\")\n",
    "    df['url_cnt'] = df['body'].apply(count_urls)\n",
    "    df['exc_cnt'] = df['body'].apply(count_exclaims)\n",
    "    df['q_cnt'] = df['body'].apply(count_questions)\n",
    "    df['upper_rt'] = df['body'].apply(upper_ratio)\n",
    "    df['rep_run'] = df['body'].apply(repeat_char_max)\n",
    "    \n",
    "    print(\"ğŸ˜Š ì´ëª¨ì§€ ë° íŠ¹ìˆ˜ë¬¸ì íŠ¹ì§• ìƒì„± ì¤‘...\")\n",
    "    emoji_set = set(\"ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ˜…ğŸ˜‚ğŸ¤£ğŸ¥²ğŸ˜ŠğŸ™‚ğŸ™ƒğŸ˜‰ğŸ˜ğŸ˜˜ğŸ˜—ğŸ˜™ğŸ˜šğŸ˜‹ğŸ˜›ğŸ˜ğŸ˜œğŸ¤ªğŸ¤¨ğŸ§ğŸ¤“ğŸ˜ğŸ¤©ğŸ¥³ğŸ˜ğŸ˜’ğŸ˜ğŸ˜”ğŸ˜ŸğŸ˜•ğŸ™â˜¹ï¸ğŸ˜£ğŸ˜–ğŸ˜«ğŸ˜©ğŸ˜¤ğŸ˜ ğŸ˜¡ğŸ¤¬\")\n",
    "    df['emoji_rt'] = df['body'].apply(\n",
    "        lambda s: sum(1 for ch in str(s) if ch in emoji_set) / max(len(str(s)), 1)\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ“š ì–´íœ˜ ê¸°ë°˜ íŠ¹ì§• ìƒì„± ì¤‘...\")\n",
    "    profanity_words = {'idiot', 'moron', 'stupid', 'dumb', 'retard', 'asshole', \n",
    "                      'bastard', 'fuck', 'shit', 'damn', 'bitch'}\n",
    "    ad_words = {'free', 'win', 'offer', 'discount', 'promo', 'sale', 'subscribe',\n",
    "                'click', 'visit', 'buy', 'deal', 'coupon', 'limited'}\n",
    "    \n",
    "    df['has_profanity'] = df['body'].apply(\n",
    "        lambda x: int(any(word in str(x).lower() for word in profanity_words))\n",
    "    )\n",
    "    df['has_adword'] = df['body'].apply(\n",
    "        lambda x: int(any(word in str(x).lower() for word in ad_words))\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ”— ê·œì¹™-ëŒ“ê¸€ ìƒí˜¸ì‘ìš© íŠ¹ì§• ìƒì„± ì¤‘...\")\n",
    "    df['rule_body_jaccard'] = [\n",
    "        jaccard_similarity(rule, body) \n",
    "        for rule, body in zip(df['rule'], df['body'])\n",
    "    ]\n",
    "    \n",
    "    print(\"âœ… íŠ¹ì§• ìƒì„± ì™„ë£Œ!\")\n",
    "    return df\n",
    "\n",
    "# ==========================================================\n",
    "# ğŸ¤– Cross-Encoder ì…ë ¥ ì¤€ë¹„ í•¨ìˆ˜\n",
    "def prepare_cross_encoder_input(rule, body, positive_ex1=None, positive_ex2=None, \n",
    "                               negative_ex1=None, negative_ex2=None):\n",
    "    \"\"\"Cross-Encoderë¥¼ ìœ„í•œ ì…ë ¥ í…ìŠ¤íŠ¸ ì¤€ë¹„\"\"\"\n",
    "    rule_text = str(rule).strip()\n",
    "    comment_text = str(body).strip()\n",
    "    \n",
    "    examples_text = \"\"\n",
    "    if pd.notna(positive_ex1) and str(positive_ex1).strip():\n",
    "        examples_text += f\" [ê¸ì •ì˜ˆì‹œ] {str(positive_ex1).strip()}\"\n",
    "    if pd.notna(negative_ex1) and str(negative_ex1).strip():\n",
    "        examples_text += f\" [ë¶€ì •ì˜ˆì‹œ] {str(negative_ex1).strip()}\"\n",
    "    \n",
    "    full_input = f\"[ê·œì¹™] {rule_text}{examples_text} [ëŒ“ê¸€] {comment_text}\"\n",
    "    return full_input\n",
    "\n",
    "# ==========================================================\n",
    "# ğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì§• ìƒì„±\n",
    "print(\"ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...\")\n",
    "\n",
    "# íŠ¹ì§• ìƒì„± ì‹¤í–‰\n",
    "train_df = create_features(train_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb6d6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Cross-Encoder ì…ë ¥ ë°ì´í„° ì¤€ë¹„ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ ë°ì´í„° ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2029/2029 [00:00<00:00, 17049.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 2029ê°œì˜ Cross-Encoder ì…ë ¥ ìŒ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ìˆ˜ì¹˜ íŠ¹ì§• ì •ì˜\n",
    "numerical_cols = [\n",
    "    'body_len', 'rule_len', 'body_words', 'url_cnt', 'exc_cnt', 'q_cnt',\n",
    "    'upper_rt', 'rep_run', 'emoji_rt', 'has_profanity', 'has_adword', \n",
    "    'rule_body_jaccard'\n",
    "]\n",
    "\n",
    "# ìˆ˜ì¹˜ íŠ¹ì§• ì •ê·œí™”\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "numerical_features = scaler.fit_transform(train_df[numerical_cols])\n",
    "\n",
    "# Cross-Encoder ì…ë ¥ ë°ì´í„° ì¤€ë¹„\n",
    "print(\"ğŸ”„ Cross-Encoder ì…ë ¥ ë°ì´í„° ì¤€ë¹„ ì¤‘...\")\n",
    "ce_inputs = []\n",
    "labels = []\n",
    "\n",
    "for idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"ì…ë ¥ ë°ì´í„° ì²˜ë¦¬\"):\n",
    "    ce_input = prepare_cross_encoder_input(\n",
    "        row['rule'], \n",
    "        row['body'],\n",
    "        row.get('positive_example_1'),\n",
    "        row.get('positive_example_2'),\n",
    "        row.get('negative_example_1'),\n",
    "        row.get('negative_example_2')\n",
    "    )\n",
    "    ce_inputs.append(ce_input)\n",
    "    labels.append(int(row['rule_violation']))\n",
    "\n",
    "ce_inputs = np.array(ce_inputs)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"âœ… {len(ce_inputs)}ê°œì˜ Cross-Encoder ì…ë ¥ ìŒ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bea179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ï¸ ì „ì²´ ë°ì´í„°ì…‹ìœ¼ë¡œ ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì‹œì‘!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š ì „ì²´ í›ˆë ¨ ì˜ˆì‹œ ìƒì„± ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ìµœì¢… í›ˆë ¨ ë°ì´í„° ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2029/2029 [00:00<00:00, 156042.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ìµœì¢… ëª¨ë¸ í›ˆë ¨ (ì—í­: 4, ë°°ì¹˜: 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [03:44<00:00,  1.76s/it]\n",
      "Epoch:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [03:44<11:12, 224.10s/it]"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# ğŸ—ï¸ ìµœì¢… ëª¨ë¸ í›ˆë ¨ (ì „ì²´ ë°ì´í„° ì‚¬ìš©)\n",
    "from sentence_transformers import CrossEncoder, InputExample\n",
    "\n",
    "print(\"ğŸ—ï¸ ì „ì²´ ë°ì´í„°ì…‹ìœ¼ë¡œ ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì‹œì‘!\")\n",
    "\n",
    "# ìµœì¢… ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model_name = 'microsoft/deberta-v3-small'\n",
    "final_model = CrossEncoder(model_name, num_labels=1, device=device)\n",
    "\n",
    "# ì „ì²´ í›ˆë ¨ ë°ì´í„°ë¡œ ì˜ˆì‹œ ìƒì„±\n",
    "print(\"ğŸ“š ì „ì²´ í›ˆë ¨ ì˜ˆì‹œ ìƒì„± ì¤‘...\")\n",
    "train_examples = []\n",
    "\n",
    "for i in tqdm(range(len(ce_inputs)), desc=\"ìµœì¢… í›ˆë ¨ ë°ì´í„° ì²˜ë¦¬\"):\n",
    "    ce_input = ce_inputs[i]\n",
    "    \n",
    "    if '[ëŒ“ê¸€]' in ce_input:\n",
    "        rule_part = ce_input.split('[ëŒ“ê¸€]')[0].strip()\n",
    "        comment_part = ce_input.split('[ëŒ“ê¸€]')[1].strip()\n",
    "    else:\n",
    "        parts = ce_input.split()\n",
    "        mid = len(parts) // 2\n",
    "        rule_part = ' '.join(parts[:mid])\n",
    "        comment_part = ' '.join(parts[mid:])\n",
    "    \n",
    "    train_examples.append(\n",
    "        InputExample(texts=[rule_part, comment_part], label=float(labels[i]))\n",
    "    )\n",
    "\n",
    "# ìµœì¢… í›ˆë ¨ ì„¤ì •\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "warmup_steps = max(1, int(len(train_dataloader) * 0.1))\n",
    "\n",
    "print(f\"ğŸš€ ìµœì¢… ëª¨ë¸ í›ˆë ¨ (ì—í­: 4, ë°°ì¹˜: 16)\")\n",
    "\n",
    "# ìµœì¢… ëª¨ë¸ í›ˆë ¨\n",
    "final_model.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    epochs=4,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path='/kaggle/working/final_cross_encoder_model',\n",
    "    save_best_model=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# ğŸ’¾ ëª¨ë¸ ë° ì „ì²˜ë¦¬ ê°ì²´ ì €ì¥\n",
    "print(\"ğŸ’¾ ëª¨ë¸ ë° ì „ì²˜ë¦¬ ê°ì²´ ì €ì¥ ì¤‘...\")\n",
    "\n",
    "# 1. Cross-Encoder ëª¨ë¸ì€ ì´ë¯¸ output_pathì— ì €ì¥ë¨\n",
    "print(\"âœ… Cross-Encoder ëª¨ë¸ ì €ì¥ ì™„ë£Œ: /kaggle/working/final_cross_encoder_model\")\n",
    "\n",
    "# 2. Scaler ì €ì¥\n",
    "joblib.dump(scaler, '/kaggle/working/scaler.pkl')\n",
    "print(\"âœ… Scaler ì €ì¥ ì™„ë£Œ: /kaggle/working/scaler.pkl\")\n",
    "\n",
    "# 3. ìˆ˜ì¹˜ íŠ¹ì§• ì»¬ëŸ¼ëª… ì €ì¥\n",
    "with open('/kaggle/working/numerical_cols.pkl', 'wb') as f:\n",
    "    pickle.dump(numerical_cols, f)\n",
    "print(\"âœ… ìˆ˜ì¹˜ íŠ¹ì§• ì»¬ëŸ¼ëª… ì €ì¥ ì™„ë£Œ: /kaggle/working/numerical_cols.pkl\")\n",
    "\n",
    "# 4. íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ í•¨ìˆ˜ë“¤ì„ í¬í•¨í•œ ìœ í‹¸ë¦¬í‹° ì €ì¥\n",
    "feature_utils = {\n",
    "    'count_urls': count_urls,\n",
    "    'count_exclaims': count_exclaims,\n",
    "    'count_questions': count_questions,\n",
    "    'upper_ratio': upper_ratio,\n",
    "    'repeat_char_max': repeat_char_max,\n",
    "    'jaccard_similarity': jaccard_similarity,\n",
    "    'create_features': create_features,\n",
    "    'prepare_cross_encoder_input': prepare_cross_encoder_input\n",
    "}\n",
    "\n",
    "with open('/kaggle/working/feature_utils.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_utils, f)\n",
    "print(\"âœ… íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ í•¨ìˆ˜ë“¤ ì €ì¥ ì™„ë£Œ: /kaggle/working/feature_utils.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffad6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# ğŸ“‹ ì €ì¥ëœ íŒŒì¼ë“¤ í™•ì¸\n",
    "print(\"\\nğŸ“‹ ì €ì¥ëœ íŒŒì¼ë“¤:\")\n",
    "for file in os.listdir('/kaggle/working'):\n",
    "    if file.endswith(('.pkl', '.bin', '.json', '.txt')) or 'model' in file:\n",
    "        file_path = f'/kaggle/working/{file}'\n",
    "        file_size = os.path.getsize(file_path) / (1024*1024)  # MB\n",
    "        print(f\"  - {file}: {file_size:.2f} MB\")\n",
    "\n",
    "print(f\"\\nğŸ‰ ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“¦ ì´ì œ ì´ íŒŒì¼ë“¤ì„ Kaggle Datasetìœ¼ë¡œ ì—…ë¡œë“œí•˜ì—¬ ì¶”ë¡  ë…¸íŠ¸ë¶ì—ì„œ ì‚¬ìš©í•˜ì„¸ìš”.\")\n",
    "print(f\"ğŸ“ ì—…ë¡œë“œí•  í´ë”: /kaggle/working/\")\n",
    "\n",
    "# ì €ì¥ëœ ëª¨ë¸ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸\n",
    "print(\"\\nğŸ§ª ì €ì¥ëœ ëª¨ë¸ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸...\")\n",
    "test_rule = \"No spam or promotional content\"\n",
    "test_body = \"Check out this amazing deal! Buy now!\"\n",
    "\n",
    "test_input = prepare_cross_encoder_input(test_rule, test_body)\n",
    "if '[ëŒ“ê¸€]' in test_input:\n",
    "    rule_part = test_input.split('[ëŒ“ê¸€]')[0].strip()\n",
    "    comment_part = test_input.split('[ëŒ“ê¸€]')[1].strip()\n",
    "else:\n",
    "    parts = test_input.split()\n",
    "    mid = len(parts) // 2\n",
    "    rule_part = ' '.join(parts[:mid])\n",
    "    comment_part = ' '.join(parts[mid:])\n",
    "\n",
    "test_pred = final_model.predict([[rule_part, comment_part]])\n",
    "test_prob = torch.sigmoid(torch.tensor(test_pred)).item()\n",
    "\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ê·œì¹™: {test_rule}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ëŒ“ê¸€: {test_body}\")\n",
    "print(f\"ìœ„ë°˜ í™•ë¥ : {test_prob:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
