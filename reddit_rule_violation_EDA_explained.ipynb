{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b601c1a0",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§­ Reddit Rule Violation: Researchâ€‘Backed EDA â†’ Baseline Modeling Notebook\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **Kaggle \"Jigsaw - Agile Community Rules Classification\"** ê³¼ ìœ ì‚¬í•œ ê³¼ì œ ë§¥ë½ì—ì„œ, ì œê³µëœ `train.csv`(comment, rule ìŒ) ê¸°ë°˜ìœ¼ë¡œ **ì—°êµ¬ ê·¼ê±°ë¥¼ ë°˜ì˜í•œ EDA**ë¥¼ ìˆ˜í–‰í•˜ê³ , **ì¸ì‚¬ì´íŠ¸ â†’ íŠ¹ì§• ì„¤ê³„ â†’ ë² ì´ìŠ¤ë¼ì¸ í•™ìŠµ**ê¹Œì§€ **íë¦„ ìžˆê²Œ** ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì°¸ê³ /ë°°ê²½ (ì—°êµ¬ & ë ˆí¼ëŸ°ìŠ¤)\n",
    "- **Rule-based moderation with LLMs**: LLMì´ ì„œë¸Œë ˆë”§ ê·œì¹™ì„ í”„ë¡¬í”„íŠ¸ë¡œ ë°›ì•„ ê·œì¹™ ìœ„ë°˜ ì—¬ë¶€ë¥¼ ì¶”ë¡ í•˜ëŠ” ì—°êµ¬. ì»¤ë®¤ë‹ˆí‹°ë³„ ì„±ëŠ¥ íŽ¸ì°¨ê°€ ì¡´ìž¬í•˜ë©° **ë£° í…ìŠ¤íŠ¸ì™€ ëŒ“ê¸€ í…ìŠ¤íŠ¸ì˜ ìƒí˜¸ìž‘ìš©(ìŒ ìž…ë ¥)**ì´ í•µì‹¬ìž„.  \n",
    "  - Kumar, AbuHashem, Durumeric (2023/2024): *Watch Your Language: Investigating Content Moderation with LLMs*. (ICWSM 2024)  \n",
    "- **ë ˆë”§ ëª¨ë”ë ˆì´ì…˜ ë°ì´í„°ì…‹/ê·œë²” ì—°êµ¬**: ë ˆë”§ì—ì„œ ì‚­ì œ/ì œê±°ëœ ìˆ˜ë°±ë§Œ ì½”ë©˜íŠ¸ ë¶„ì„, ì»¤ë®¤ë‹ˆí‹° ê·œë²”(macro/micro) ìœ„ë°˜ íƒì§€. **ìŠ¤íŒ¸/ê´‘ê³ , ì¸ì‹ ê³µê²©, ì •ì¹˜/ê·œì¹™ ìœ„ë°˜** ë“± ì´ì§ˆì  ê·œë²” ì¡´ìž¬.  \n",
    "  - Chandrasekharan & Gilbert (2018/2019): *Norm Violations*, *Hybrid Approaches*, *Crossmod* ë“±.\n",
    "- **ì‹¤ë¬´ì  ë² ì´ìŠ¤ë¼ì¸**: í…ìŠ¤íŠ¸ ë¶„ë¥˜ì—ì„œ **TFâ€‘IDF + ë¡œì§€ìŠ¤í‹± íšŒê·€**ê°€ ë‚®ì€ ì—°ì‚°ë¹„ë¡œ ê°•í•œ ì„±ëŠ¥Â·AUC í™•ë³´. Toxic/abuse íƒì§€ ë²¤ì¹˜ë§ˆí¬ì—ì„œë„ ì¼ê´€ë˜ê²Œ ê°•í•¨.\n",
    "- **ìŠ¤íŒ¸/ê´‘ê³  ì‹ í˜¸**: URL ìˆ˜, ë„ë©”ì¸, ë°˜ë³µë¬¸ìž, ê³¼ë„í•œ ëŒ€ë¬¸ìž/êµ¬ë‘ì , ê¸¸ì´, ì´ëª¨ì§€ ë¹„ìœ¨ ë“± **ìŠ¤íƒ€ì¼ íŠ¹ì§•**ì´ ìœ ìš©í•¨.\n",
    "\n",
    "> ë³¸ ë…¸íŠ¸ë¶ì€ ìœ„ ê·¼ê±°ë¥¼ ë°˜ì˜í•´ **(1) ë°ì´í„° í’ˆì§ˆ/ë¶„í¬ ì ê²€ â†’ (2) ê·œì¹™/ì»¤ë®¤ë‹ˆí‹°/í…ìŠ¤íŠ¸ íŠ¹ì„± ë¶„ì„ â†’ (3) ë£°-ë³¸ë¬¸ ìŒ ìƒí˜¸ìž‘ìš© íŠ¹ì§•**(ìœ ì‚¬ë„, TFâ€‘IDF ìŒ ì¸ì½”ë”© ë“±) â†’ (4) **ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸(AUC ê²€ì¦)** â†’ (5) ê°œì„  ë¡œë“œë§µ ìˆœìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e7ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, math, string, unicodedata, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "DATA_PATH = '/mnt/data/train.csv' if os.path.exists('/mnt/data/train.csv') else 'train.csv'\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c6dbeb",
   "metadata": {},
   "source": [
    "## 1) ë°ì´í„° ê°œìš” / ë¬´ê²°ì„± ì ê²€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664e1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.sample(5, random_state=42))\n",
    "display(df.describe(include='all'))\n",
    "print(\"\\nNull counts:\\n\", df.isnull().sum())\n",
    "print(\"\\nDtypes:\\n\", df.dtypes)\n",
    "dup_pairs = df.duplicated(subset=['body','rule']).sum()\n",
    "print(f\"Duplicated (body, rule) pairs: {dup_pairs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8732a279",
   "metadata": {},
   "source": [
    "## 2) íƒ€ê¹ƒ ë¶„í¬ (`rule_violation`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ad205",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target = 'rule_violation'\n",
    "vc = df[target].value_counts().sort_index()\n",
    "print(vc)\n",
    "print(\"\\nClass ratio:\", (vc / vc.sum()).to_dict())\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.bar(vc.index.astype(str), vc.values)\n",
    "plt.title('Rule Violation Distribution')\n",
    "plt.xlabel('rule_violation')\n",
    "plt.ylabel('count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad6a53d",
   "metadata": {},
   "source": [
    "## 3) Subreddit ë¶„í¬ ë° ê·œì¹™ ìœ„ë°˜ìœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5fd157",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sub_vc = df['subreddit'].value_counts()\n",
    "print(\"Unique subreddits:\", df['subreddit'].nunique())\n",
    "display(sub_vc.head(20))\n",
    "\n",
    "sub_stats = (df.groupby('subreddit')[target]\n",
    "               .agg(['mean','count'])\n",
    "               .rename(columns={'mean':'violation_rate','count':'n'})\n",
    "               .sort_values('n', ascending=False))\n",
    "display(sub_stats.head(20))\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "top_counts = sub_vc.head(20)\n",
    "plt.barh(range(len(top_counts)), top_counts.values)\n",
    "plt.yticks(range(len(top_counts)), top_counts.index)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Top 20 Subreddits (by count)')\n",
    "plt.xlabel('count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "top_subs = sub_stats.head(20)\n",
    "plt.barh(range(len(top_subs)), top_subs['violation_rate'].values)\n",
    "plt.yticks(range(len(top_subs)), top_subs.index)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Top 20 Subreddits - Violation Rate')\n",
    "plt.xlabel('violation_rate')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d807f7ba",
   "metadata": {},
   "source": [
    "## 4) Rule(ê·œì¹™) ë¶„í¬ & ìœ„ë°˜ìœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a8ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rule_vc = df['rule'].value_counts()\n",
    "print(\"Unique rules:\", df['rule'].nunique())\n",
    "display(rule_vc.head(15))\n",
    "\n",
    "rule_stats = (df.groupby('rule')[target]\n",
    "                .agg(['mean','count'])\n",
    "                .rename(columns={'mean':'violation_rate','count':'n'})\n",
    "                .sort_values('n', ascending=False))\n",
    "display(rule_stats.head(20))\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "top_rules = rule_vc.head(15)\n",
    "plt.barh(range(len(top_rules)), top_rules.values)\n",
    "plt.yticks(range(len(top_rules)), top_rules.index)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Top 15 Rules (by count)')\n",
    "plt.xlabel('count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "top_rules_stats = rule_stats.head(15)\n",
    "plt.barh(range(len(top_rules_stats)), top_rules_stats['violation_rate'].values)\n",
    "plt.yticks(range(len(top_rules_stats)), top_rules_stats.index)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Top 15 Rules - Violation Rate')\n",
    "plt.xlabel('violation_rate')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6556a546",
   "metadata": {},
   "source": [
    "## 5) ë³¸ë¬¸ ê¸¸ì´/ê¸°ì´ˆ í†µê³„ (ìŠ¤íŒ¸Â·ê·œë²” ìœ„ë°˜ ê´€ë ¨ ì‹ í˜¸)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888d89dd",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Lexicon ê¸°ë°˜ íŠ¹ì§•ì´ëž€?\n",
    "Lexiconì€ íŠ¹ì • ì£¼ì œ(ì˜ˆ: ìš•ì„¤, ê´‘ê³ )ì™€ ê´€ë ¨ëœ **ë‹¨ì–´ ì‚¬ì „**ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.  \n",
    "EDAì—ì„œ `url_cnt`, `upper_rt`, `exc_cnt` ê°™ì€ ì»¬ëŸ¼ì€ ë‹¨ì–´ ì‚¬ì „ì´ë‚˜ ìŠ¤íƒ€ì¼ ê·œì¹™ì— ê¸°ë°˜í•´ ìƒì„±ëœ íŠ¹ì§•ìž…ë‹ˆë‹¤.  \n",
    "ì¦‰, **ë³¸ë¬¸ ìžì²´ ì˜ë¯¸ë³´ë‹¤ ê¸€ì“°ê¸° íŒ¨í„´(ìŠ¤íƒ€ì¼)** ì„ ì´ìš©í•´ ê·œì¹™ ìœ„ë°˜ì„ íƒì§€í•˜ëŠ” ë³´ì¡° ì‹ í˜¸ìž…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc14494",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_urls(text):\n",
    "    return len(re.findall(r'https?://\\S+|www\\.\\S+', str(text)))\n",
    "\n",
    "def count_exclaims(text):\n",
    "    return str(text).count('!')\n",
    "\n",
    "def count_questions(text):\n",
    "    return str(text).count('?')\n",
    "\n",
    "def upper_ratio(text):\n",
    "    s = str(text)\n",
    "    letters = [c for c in s if c.isalpha()]\n",
    "    if not letters:\n",
    "        return 0.0\n",
    "    upp = sum(1 for c in letters if c.isupper())\n",
    "    return upp / len(letters)\n",
    "\n",
    "def repeat_char_max(text):\n",
    "    longest = 1\n",
    "    last = ''\n",
    "    cur = 0\n",
    "    for ch in str(text):\n",
    "        if ch == last:\n",
    "            cur += 1\n",
    "        else:\n",
    "            longest = max(longest, cur)\n",
    "            cur = 1\n",
    "            last = ch\n",
    "    longest = max(longest, cur)\n",
    "    return longest\n",
    "\n",
    "def emoji_ratio(text):\n",
    "    s = str(text)\n",
    "    total = len(s) if len(s)>0 else 1\n",
    "    emojis = sum(1 for ch in s if ch in emoji_set)\n",
    "    return emojis / total\n",
    "\n",
    "emoji_set = set(list(\"ðŸ˜€ðŸ˜ƒðŸ˜„ðŸ˜ðŸ˜†ðŸ˜…ðŸ˜‚ðŸ¤£ðŸ¥²ðŸ˜ŠðŸ™‚ðŸ™ƒðŸ˜‰ðŸ˜ðŸ˜˜ðŸ˜—ðŸ˜™ðŸ˜šðŸ˜‹ðŸ˜›ðŸ˜ðŸ˜œðŸ¤ªðŸ¤¨ðŸ§ðŸ¤“ðŸ˜ŽðŸ¤©ðŸ¥³ðŸ˜ðŸ˜’ðŸ˜žðŸ˜”ðŸ˜ŸðŸ˜•ðŸ™â˜¹ï¸ðŸ˜£ðŸ˜–ðŸ˜«ðŸ˜©ðŸ˜¤ðŸ˜ ðŸ˜¡ðŸ¤¬\"))\n",
    "\n",
    "df['body_len'] = df['body'].astype(str).str.len()\n",
    "df['url_cnt'] = df['body'].apply(count_urls)\n",
    "df['exc_cnt'] = df['body'].apply(count_exclaims)\n",
    "df['q_cnt']   = df['body'].apply(count_questions)\n",
    "df['upper_rt'] = df['body'].apply(upper_ratio)\n",
    "df['rep_run']  = df['body'].apply(repeat_char_max)\n",
    "df['emoji_rt'] = df['body'].apply(emoji_ratio)\n",
    "\n",
    "display(df[['body_len','url_cnt','exc_cnt','q_cnt','upper_rt','rep_run','emoji_rt', 'rule_violation']].describe())\n",
    "\n",
    "fig, axes = plt.subplots(2,3, figsize=(12,7)); axes = axes.ravel()\n",
    "axes[0].hist(df['body_len'], bins=50); axes[0].set_title('body_len')\n",
    "axes[1].hist(df['url_cnt'], bins=20);  axes[1].set_title('url_cnt')\n",
    "axes[2].hist(df['upper_rt'], bins=30); axes[2].set_title('upper_rt')\n",
    "axes[3].hist(df['exc_cnt'], bins=20);  axes[3].set_title('exc_cnt')\n",
    "axes[4].hist(df['q_cnt'], bins=20);    axes[4].set_title('q_cnt')\n",
    "axes[5].hist(df['rep_run'], bins=20);  axes[5].set_title('rep_run')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "for m in ['body_len','url_cnt','upper_rt','exc_cnt','q_cnt','rep_run','emoji_rt']:\n",
    "    plt.figure(figsize=(4,3))\n",
    "    data0 = df.loc[df[target]==0, m].values\n",
    "    data1 = df.loc[df[target]==1, m].values\n",
    "    plt.boxplot([data0, data1], labels=['non-viol','viol'])\n",
    "    plt.title(m)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "corr = df[['body_len','url_cnt','upper_rt','exc_cnt','q_cnt','rep_run','emoji_rt', target]].corr(numeric_only=True)[target].sort_values(ascending=False)\n",
    "print(\"Correlation with target (pearson):\\n\", corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fd10af",
   "metadata": {},
   "source": [
    "## 6) ê²½ëŸ‰ Lexicon ê¸°ë°˜ ì‹ í˜¸ (ìš•ì„¤/ê´‘ê³ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc05a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "profanity = {'idiot','moron','stupid','dumb','retard','asshole','bastard'}\n",
    "ad_words  = {'free','win','offer','discount','promo','sale','subscribe','click','visit','buy','deal','coupon','%off'}\n",
    "\n",
    "def contains_any(text, vocab):\n",
    "    s = str(text).lower()\n",
    "    return int(any(tok in s for tok in vocab))\n",
    "\n",
    "df['has_profanity'] = df['body'].apply(lambda x: contains_any(x, profanity))\n",
    "df['has_adword']    = df['body'].apply(lambda x: contains_any(x, ad_words))\n",
    "\n",
    "print(df[['has_profanity','has_adword', target]].groupby(['has_profanity','has_adword']).agg(['mean','count']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dce80d4",
   "metadata": {},
   "source": [
    "## 7) ê·œì¹™ í…ìŠ¤íŠ¸ vs ë³¸ë¬¸ ìƒí˜¸ìž‘ìš© (ìœ ì‚¬ë„ íŠ¹ì§•)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1732a905",
   "metadata": {},
   "source": [
    "### ðŸ“ Jaccard ìœ ì‚¬ë„\n",
    "Jaccard ìœ ì‚¬ë„ëŠ” **ë‘ ì§‘í•©ì˜ êµì§‘í•© ë¹„ìœ¨**ë¡œ ì •ì˜ë©ë‹ˆë‹¤.  \n",
    "âž¡ ê·œì¹™ í…ìŠ¤íŠ¸ì™€ ëŒ“ê¸€ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ ì§‘í•©ìœ¼ë¡œ ë°”ê¿”, ê²¹ì¹˜ëŠ” ë‹¨ì–´ ë¹„ìœ¨ì„ ì¸¡ì •í•©ë‹ˆë‹¤.  \n",
    "\n",
    "- ê°’ ë²”ìœ„: 0 (ê²¹ì¹¨ ì—†ìŒ) ~ 1 (ì™„ì „ížˆ ë™ì¼)\n",
    "- ê·œì¹™ ìœ„ë°˜ ëŒ“ê¸€ì¼ìˆ˜ë¡ ê·œì¹™ ë¬¸êµ¬ì™€ ë‹¨ì–´ê°€ ê²¹ì¹  í™•ë¥ ì´ ë†’ì•„, Jaccard ê°’ì´ ë†’ì•„ì§‘ë‹ˆë‹¤.\n",
    "\n",
    "ðŸ‘‰ ë”°ë¼ì„œ ê·œì¹™ ìœ„ë°˜ íƒì§€ì—ì„œ **ê·œì¹™-ë³¸ë¬¸ ìƒí˜¸ìž‘ìš©**ì„ ë°˜ì˜í•˜ëŠ” ì¤‘ìš”í•œ ì‹ í˜¸ìž…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da7312",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jaccard(a, b):\n",
    "    sa, sb = set(a.split()), set(b.split())\n",
    "    u = len(sa|sb); i = len(sa&sb)\n",
    "    return i / u if u else 0.0\n",
    "\n",
    "df['rule_body_jaccard'] = [jaccard(r, b) for r,b in zip(df['rule'].astype(str), df['body'].astype(str))]\n",
    "print(\"rule_body_jaccard head:\\n\", df['rule_body_jaccard'].head())\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.hist(df['rule_body_jaccard'], bins=30)\n",
    "plt.title('Jaccard(rule, body)')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.boxplot([df.loc[df[target]==0, 'rule_body_jaccard'],\n",
    "             df.loc[df[target]==1, 'rule_body_jaccard']], labels=['non-viol','viol'])\n",
    "plt.title('Jaccard vs target')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec573b",
   "metadata": {},
   "source": [
    "## 8) ì œê³µëœ ì˜ˆì‹œ í…ìŠ¤íŠ¸(positive/negative example)ì™€ì˜ ìœ ì‚¬ë„/ëˆ„ì„¤ ì ê²€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4327a0",
   "metadata": {},
   "source": [
    "### ðŸš¨ ë°ì´í„° ëˆ„ì„¤(Leakage) ì ê²€\n",
    "ê·œì¹™ ì˜ˆì‹œ í…ìŠ¤íŠ¸(`positive_example_1` ë“±)ê°€ ëŒ“ê¸€ ë³¸ë¬¸ì— í¬í•¨ë˜ëŠ” ê²½ìš°,  \n",
    "ëª¨ë¸ì€ 'ì˜ˆì‹œ ë¬¸êµ¬=ì •ë‹µ'ì´ë¼ëŠ” íŽ¸ë²•ì„ í•™ìŠµí•´ë²„ë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.  \n",
    "ë”°ë¼ì„œ `body_contains_xxx` ì»¬ëŸ¼ì„ ë§Œë“¤ì–´ ì‹¤ì œ í¬í•¨ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê³ , íƒ€ê¹ƒê³¼ì˜ ìƒê´€ì„ ê²€í† í•©ë‹ˆë‹¤.  \n",
    "ì´ëŠ” **ë°ì´í„°ì…‹ ì„¤ê³„ìƒ ëˆ„ì„¤ ì—¬ë¶€ë¥¼ ê²€ì¦í•˜ëŠ” í•µì‹¬ ì ˆì°¨**ìž…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ae897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ex_cols = ['positive_example_1','positive_example_2','negative_example_1','negative_example_2']\n",
    "for col in ex_cols:\n",
    "    df[f'body_contains_{col}'] = df.apply(lambda r: int(str(r[col])[:120].lower() in str(r['body']).lower()), axis=1)\n",
    "    print(col, df[f'body_contains_{col}'].sum())\n",
    "\n",
    "leak_cols = [c for c in df.columns if c.startswith('body_contains_')]\n",
    "display(df[leak_cols + [target]].groupby(leak_cols)[target].agg(['mean','count']).sort_values('count', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f55c6ae",
   "metadata": {},
   "source": [
    "## 9) ëª¨ë¸ë§ ìž…ë ¥ êµ¬ì„± (í…ìŠ¤íŠ¸ + ìŠ¤íƒ€ì¼ + ìƒí˜¸ìž‘ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de505180",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['rule_sep_body'] = df['rule'].astype(str) + ' [SEP] ' + df['body'].astype(str)\n",
    "num_cols = ['body_len','url_cnt','exc_cnt','q_cnt','upper_rt','rep_run','emoji_rt','has_profanity','has_adword','rule_body_jaccard']\n",
    "X_text = df['rule_sep_body']; X_num = df[num_cols]\n",
    "y = df[target].astype(int)\n",
    "display(X_num.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e8909",
   "metadata": {},
   "source": [
    "## 10) ë² ì´ìŠ¤ë¼ì¸: TFâ€‘IDF(word+char) + ë¡œì§€ìŠ¤í‹± íšŒê·€ (Stratified 5â€‘Fold AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3489e80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "tf_word = TfidfVectorizer(strip_accents='unicode', lowercase=True, ngram_range=(1,2), min_df=2, max_features=40000)\n",
    "tf_char = TfidfVectorizer(analyzer='char', ngram_range=(3,5), min_df=2, max_features=60000)\n",
    "scaler  = StandardScaler(with_mean=False)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_pred = np.zeros(len(df), dtype=float)\n",
    "fold_scores = []\n",
    "\n",
    "for fold, (tr, va) in enumerate(skf.split(X_text, y), 1):\n",
    "    Xw_tr = tf_word.fit_transform(X_text.iloc[tr]); Xw_va = tf_word.transform(X_text.iloc[va])\n",
    "    Xc_tr = tf_char.fit_transform(X_text.iloc[tr]); Xc_va = tf_char.transform(X_text.iloc[va])\n",
    "    Xn_tr = scaler.fit_transform(X_num.iloc[tr]);   Xn_va = scaler.transform(X_num.iloc[va])\n",
    "    X_tr = hstack([Xw_tr, Xc_tr, Xn_tr], format='csr')\n",
    "    X_va = hstack([Xw_va, Xc_va, Xn_va], format='csr')\n",
    "    clf = LogisticRegression(solver='saga', max_iter=3000, n_jobs=-1, class_weight='balanced', C=2.0, penalty='l2')\n",
    "    clf.fit(X_tr, y.iloc[tr])\n",
    "    oof_pred[va] = clf.predict_proba(X_va)[:,1]\n",
    "    auc = roc_auc_score(y.iloc[va], oof_pred[va])\n",
    "    fold_scores.append(auc)\n",
    "    print(f\"[Fold {fold}] AUC = {auc:.4f}\")\n",
    "print(\"\\nCV AUC:\", np.mean(fold_scores).round(4), \"+/-\", np.std(fold_scores).round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb59c8",
   "metadata": {},
   "source": [
    "## 11) ë‹¨ì–´ ì¤‘ìš”ë„ ì‚´íŽ´ë³´ê¸° (ì°¸ê³ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39704401",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    word_feats = np.array(tf_word.get_feature_names_out())\n",
    "    coefs = clf.coef_[0][:len(word_feats)]\n",
    "    idx_top = np.argsort(-coefs)[:30]\n",
    "    idx_bot = np.argsort(coefs)[:30]\n",
    "    print(\"Top +coef tokens (push to violation):\")\n",
    "    print(list(zip(word_feats[idx_top], coefs[idx_top])))\n",
    "    print(\"\\nTop -coef tokens (push to non-violation):\")\n",
    "    print(list(zip(word_feats[idx_bot], coefs[idx_bot])))\n",
    "except Exception as e:\n",
    "    print(\"Feature importance preview skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c0d88b",
   "metadata": {},
   "source": [
    "\n",
    "## 12) ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ìš”ì•½ â†’ ì•¡ì…˜ ì•„ì´í…œ\n",
    "\n",
    "- **ì„œë¸Œë ˆë”§Â·ê·œì¹™ë³„ ë¶„í¬ ì°¨ì´**: ë°ì´í„°ëŠ” ì»¤ë®¤ë‹ˆí‹°/ë£°ì— ë”°ë¼ **í‘œë³¸ ìˆ˜ì™€ ìœ„ë°˜ìœ¨**ì´ ìƒì´í•©ë‹ˆë‹¤. **Stratified split** ë° **ì»¤ë®¤ë‹ˆí‹°/ë£° ë¶„í¬ ë³´ì •**ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "- **ìŠ¤íŒ¸/ê´‘ê³  ì‹ í˜¸**: URL ìˆ˜, ë°˜ë³µë¬¸ìž, ëŒ€ë¬¸ìž ë¹„ìœ¨, ê°íƒ„/ë¬¼ìŒí‘œ ê³¼ë‹¤ ë“± **ìŠ¤íƒ€ì¼ íŠ¹ì§•**ì´ ìœ„ë°˜ê³¼ ìƒê´€(ìƒì„¸ ìˆ˜ì¹˜ ìœ„ ì…€ ì°¸ê³ ).\n",
    "- **ë£°-ë³¸ë¬¸ ìƒí˜¸ìž‘ìš©**: `rule` í…ìŠ¤íŠ¸ì™€ `body`ê°„ **í† í° ê³µìœ ìœ¨(Jaccard)** ë˜ëŠ” **í•©ì³ì„œ ë²¡í„°í™”(rule [SEP] body)**ê°€ ìœ íš¨í•œ ì‹ í˜¸ë¡œ ë³´ìž…ë‹ˆë‹¤.\n",
    "- **ì˜ˆì‹œ í…ìŠ¤íŠ¸ ëˆ„ì„¤ ì ê²€**: ëŒ“ê¸€ ë³¸ë¬¸ì´ ì˜ˆì‹œ ë¬¸ìžì—´ ì¼ë¶€ë¥¼ í¬í•¨í•˜ëŠ” ì‚¬ë¡€ê°€ ìžˆëŠ”ì§€ ë°˜ë“œì‹œ í™•ì¸(ìƒê¸° ê²°ê³¼ ì°¸ê³ ). ì¡´ìž¬ ì‹œ, í•´ë‹¹ íŠ¹ì§• ì‚¬ìš©ì€ ê¸ˆì§€í•˜ê±°ë‚˜ êµì • í•„ìš”.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac70c3d4",
   "metadata": {},
   "source": [
    "## 13) ì „ì²´ ìž¬í•™ìŠµ & ì œì¶œ í•¨ìˆ˜ (test.csv ê°€ì •)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1a6f6",
   "metadata": {},
   "source": [
    "### ðŸ“ Jaccard ìœ ì‚¬ë„\n",
    "Jaccard ìœ ì‚¬ë„ëŠ” **ë‘ ì§‘í•©ì˜ êµì§‘í•© ë¹„ìœ¨**ë¡œ ì •ì˜ë©ë‹ˆë‹¤.  \n",
    "âž¡ ê·œì¹™ í…ìŠ¤íŠ¸ì™€ ëŒ“ê¸€ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ ì§‘í•©ìœ¼ë¡œ ë°”ê¿”, ê²¹ì¹˜ëŠ” ë‹¨ì–´ ë¹„ìœ¨ì„ ì¸¡ì •í•©ë‹ˆë‹¤.  \n",
    "\n",
    "- ê°’ ë²”ìœ„: 0 (ê²¹ì¹¨ ì—†ìŒ) ~ 1 (ì™„ì „ížˆ ë™ì¼)\n",
    "- ê·œì¹™ ìœ„ë°˜ ëŒ“ê¸€ì¼ìˆ˜ë¡ ê·œì¹™ ë¬¸êµ¬ì™€ ë‹¨ì–´ê°€ ê²¹ì¹  í™•ë¥ ì´ ë†’ì•„, Jaccard ê°’ì´ ë†’ì•„ì§‘ë‹ˆë‹¤.\n",
    "\n",
    "ðŸ‘‰ ë”°ë¼ì„œ ê·œì¹™ ìœ„ë°˜ íƒì§€ì—ì„œ **ê·œì¹™-ë³¸ë¬¸ ìƒí˜¸ìž‘ìš©**ì„ ë°˜ì˜í•˜ëŠ” ì¤‘ìš”í•œ ì‹ í˜¸ìž…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc408f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jaccard(a, b):\n",
    "    sa, sb = set(a.split()), set(b.split())\n",
    "    u = len(sa|sb); i = len(sa&sb)\n",
    "    return i / u if u else 0.0\n",
    "\n",
    "def fit_full_and_predict(train_df, test_df):\n",
    "    X_text_tr = train_df['rule'].astype(str) + ' [SEP] ' + train_df['body'].astype(str)\n",
    "    X_text_te = test_df['rule'].astype(str) + ' [SEP] ' + test_df['body'].astype(str)\n",
    "    y_tr = train_df['rule_violation'].astype(int)\n",
    "\n",
    "    # íŒŒìƒ ìƒì„± for test\n",
    "    def enrich(df_):\n",
    "        df_ = df_.copy()\n",
    "        df_['body_len'] = df_['body'].astype(str).str.len()\n",
    "        df_['url_cnt'] = df_['body'].apply(lambda t: len(re.findall(r'https?://\\S+|www\\.\\S+', str(t))))\n",
    "        df_['exc_cnt'] = df_['body'].apply(lambda t: str(t).count('!'))\n",
    "        df_['q_cnt']   = df_['body'].apply(lambda t: str(t).count('?'))\n",
    "        def upper_ratio(text):\n",
    "            s = str(text)\n",
    "            letters = [c for c in s if c.isalpha()]\n",
    "            if not letters:\n",
    "                return 0.0\n",
    "            upp = sum(1 for c in letters if c.isupper())\n",
    "            return upp / len(letters)\n",
    "        df_['upper_rt'] = df_['body'].apply(upper_ratio)\n",
    "        def repeat_char_max(text):\n",
    "            longest = 1; last=''; cur=0\n",
    "            for ch in str(text):\n",
    "                if ch == last: cur += 1\n",
    "                else: longest = max(longest, cur); cur=1; last=ch\n",
    "            longest = max(longest, cur)\n",
    "            return longest\n",
    "        df_['rep_run']  = df_['body'].apply(repeat_char_max)\n",
    "        emoji_set = set(list(\"ðŸ˜€ðŸ˜ƒðŸ˜„ðŸ˜ðŸ˜†ðŸ˜…ðŸ˜‚ðŸ¤£ðŸ¥²ðŸ˜ŠðŸ™‚ðŸ™ƒðŸ˜‰ðŸ˜ðŸ˜˜ðŸ˜—ðŸ˜™ðŸ˜šðŸ˜‹ðŸ˜›ðŸ˜ðŸ˜œðŸ¤ªðŸ¤¨ðŸ§ðŸ¤“ðŸ˜ŽðŸ¤©ðŸ¥³ðŸ˜ðŸ˜’ðŸ˜žðŸ˜”ðŸ˜ŸðŸ˜•ðŸ™â˜¹ï¸ðŸ˜£ðŸ˜–ðŸ˜«ðŸ˜©ðŸ˜¤ðŸ˜ ðŸ˜¡ðŸ¤¬\"))\n",
    "        df_['emoji_rt'] = df_['body'].apply(lambda s: (sum(1 for ch in str(s) if ch in emoji_set) / (len(str(s)) if len(str(s))>0 else 1)))\n",
    "        profanity = {'idiot','moron','stupid','dumb','retard','asshole','bastard'}\n",
    "        ad_words  = {'free','win','offer','discount','promo','sale','subscribe','click','visit','buy','deal','coupon','%off'}\n",
    "        df_['has_profanity'] = df_['body'].apply(lambda x: int(any(tok in str(x).lower() for tok in profanity)))\n",
    "        df_['has_adword']    = df_['body'].apply(lambda x: int(any(tok in str(x).lower() for tok in ad_words)))\n",
    "        df_['rule_body_jaccard'] = [jaccard(r, b) for r,b in zip(df_['rule'].astype(str), df_['body'].astype(str))]\n",
    "        return df_\n",
    "\n",
    "    train_df = enrich(train_df); test_df = enrich(test_df)\n",
    "\n",
    "    num_cols = ['body_len','url_cnt','exc_cnt','q_cnt','upper_rt','rep_run','emoji_rt','has_profanity','has_adword','rule_body_jaccard']\n",
    "    X_num_tr = train_df[num_cols]; X_num_te = test_df[num_cols]\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from scipy.sparse import hstack\n",
    "\n",
    "    tf_word = TfidfVectorizer(strip_accents='unicode', lowercase=True, ngram_range=(1,2), min_df=2, max_features=40000)\n",
    "    tf_char = TfidfVectorizer(analyzer='char', ngram_range=(3,5), min_df=2, max_features=60000)\n",
    "    scaler  = StandardScaler(with_mean=False)\n",
    "\n",
    "    Xw_tr = tf_word.fit_transform(X_text_tr); Xw_te = tf_word.transform(X_text_te)\n",
    "    Xc_tr = tf_char.fit_transform(X_text_tr); Xc_te = tf_char.transform(X_text_te)\n",
    "    Xn_tr = scaler.fit_transform(X_num_tr);   Xn_te = scaler.transform(X_num_te)\n",
    "\n",
    "    X_tr = hstack([Xw_tr, Xc_tr, Xn_tr], format='csr')\n",
    "    X_te = hstack([Xw_te, Xc_te, Xn_te], format='csr')\n",
    "\n",
    "    clf = LogisticRegression(solver='saga', max_iter=3000, n_jobs=-1, class_weight='balanced', C=2.0, penalty='l2')\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    proba = clf.predict_proba(X_te)[:,1]\n",
    "    return proba\n",
    "\n",
    "def make_submission(test_csv_path, out_path='submission.csv'):\n",
    "    test_df = pd.read_csv(test_csv_path)\n",
    "    probs = fit_full_and_predict(df.copy(), test_df.copy())\n",
    "    sub = pd.DataFrame({'row_id': test_df['row_id'], 'rule_violation': probs})\n",
    "    sub.to_csv(out_path, index=False)\n",
    "    print(\"Saved:\", out_path)\n",
    "    return sub\n",
    "\n",
    "print(\"Ready: call make_submission('test.csv') when test is available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b1be34",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b494f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
