{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
    "print(f\"GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e6f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# ğŸ“Š ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì •ë³´ í™•ì¸\n",
    "# ë¡œì»¬ ê²½ë¡œì—ì„œ í•™ìŠµ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "print(f\"ğŸ” ë°ì´í„° í˜•íƒœ: {train_df.shape}\")\n",
    "print(f\"ğŸ¯ íƒ€ê²Ÿ ë¶„í¬: {train_df['rule_violation'].value_counts().to_dict()}\")\n",
    "print(\"\\nğŸ“‹ ë°ì´í„° ìƒ˜í”Œ:\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66819f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# ğŸ› ï¸ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ í•¨ìˆ˜ ì •ì˜\n",
    "def count_urls(text):\n",
    "    \"\"\"URL ê°œìˆ˜ ì„¸ê¸°\"\"\"\n",
    "    return len(re.findall(r'https?://\\S+|www\\.\\S+', str(text)))\n",
    "\n",
    "def count_exclaims(text):\n",
    "    \"\"\"ê°íƒ„í‘œ ê°œìˆ˜ ì„¸ê¸°\"\"\"\n",
    "    return str(text).count('!')\n",
    "\n",
    "def count_questions(text):\n",
    "    \"\"\"ë¬¼ìŒí‘œ ê°œìˆ˜ ì„¸ê¸°\"\"\"\n",
    "    return str(text).count('?')\n",
    "\n",
    "def upper_ratio(text):\n",
    "    \"\"\"ëŒ€ë¬¸ì ë¹„ìœ¨ ê³„ì‚°\"\"\"\n",
    "    s = str(text)\n",
    "    letters = [c for c in s if c.isalpha()]\n",
    "    if not letters:\n",
    "        return 0.0\n",
    "    upp = sum(1 for c in letters if c.isupper())\n",
    "    return upp / len(letters)\n",
    "\n",
    "def repeat_char_max(text):\n",
    "    \"\"\"ì—°ì†ëœ ë¬¸ìì˜ ìµœëŒ€ ê¸¸ì´\"\"\"\n",
    "    longest = 1\n",
    "    last = ''\n",
    "    cur = 0\n",
    "    for ch in str(text):\n",
    "        if ch == last:\n",
    "            cur += 1\n",
    "        else:\n",
    "            longest = max(longest, cur)\n",
    "            cur = 1\n",
    "            last = ch\n",
    "    longest = max(longest, cur)\n",
    "    return longest\n",
    "\n",
    "def jaccard_similarity(text1, text2):\n",
    "    \"\"\"ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚°\"\"\"\n",
    "    set1 = set(str(text1).lower().split())\n",
    "    set2 = set(str(text2).lower().split())\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "# ==========================================================\n",
    "# ğŸ”§ íŠ¹ì§• ìƒì„± í•¨ìˆ˜\n",
    "def create_features(df):\n",
    "    \"\"\"EDAì—ì„œ ë°œê²¬í•œ ìœ ìš©í•œ íŠ¹ì§•ë“¤ì„ ìƒì„±\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"ğŸ“ ê¸°ë³¸ í…ìŠ¤íŠ¸ íŠ¹ì§• ìƒì„± ì¤‘...\")\n",
    "    df['body_len'] = df['body'].astype(str).str.len()\n",
    "    df['rule_len'] = df['rule'].astype(str).str.len()\n",
    "    df['body_words'] = df['body'].astype(str).str.split().str.len()\n",
    "    \n",
    "    print(\"ğŸ¨ ìŠ¤íƒ€ì¼ íŠ¹ì§• ìƒì„± ì¤‘...\")\n",
    "    df['url_cnt'] = df['body'].apply(count_urls)\n",
    "    df['exc_cnt'] = df['body'].apply(count_exclaims)\n",
    "    df['q_cnt'] = df['body'].apply(count_questions)\n",
    "    df['upper_rt'] = df['body'].apply(upper_ratio)\n",
    "    df['rep_run'] = df['body'].apply(repeat_char_max)\n",
    "    \n",
    "    print(\"ğŸ˜Š ì´ëª¨ì§€ ë° íŠ¹ìˆ˜ë¬¸ì íŠ¹ì§• ìƒì„± ì¤‘...\")\n",
    "    emoji_set = set(\"ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ˜…ğŸ˜‚ğŸ¤£ğŸ¥²ğŸ˜ŠğŸ™‚ğŸ™ƒğŸ˜‰ğŸ˜ğŸ˜˜ğŸ˜—ğŸ˜™ğŸ˜šğŸ˜‹ğŸ˜›ğŸ˜ğŸ˜œğŸ¤ªğŸ¤¨ğŸ§ğŸ¤“ğŸ˜ğŸ¤©ğŸ¥³ğŸ˜ğŸ˜’ğŸ˜ğŸ˜”ğŸ˜ŸğŸ˜•ğŸ™â˜¹ï¸ğŸ˜£ğŸ˜–ğŸ˜«ğŸ˜©ğŸ˜¤ğŸ˜ ğŸ˜¡ğŸ¤¬\")\n",
    "    df['emoji_rt'] = df['body'].apply(\n",
    "        lambda s: sum(1 for ch in str(s) if ch in emoji_set) / max(len(str(s)), 1)\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ“š ì–´íœ˜ ê¸°ë°˜ íŠ¹ì§• ìƒì„± ì¤‘...\")\n",
    "    profanity_words = {'idiot', 'moron', 'stupid', 'dumb', 'retard', 'asshole', \n",
    "                      'bastard', 'fuck', 'shit', 'damn', 'bitch'}\n",
    "    ad_words = {'free', 'win', 'offer', 'discount', 'promo', 'sale', 'subscribe',\n",
    "                'click', 'visit', 'buy', 'deal', 'coupon', 'limited'}\n",
    "    \n",
    "    df['has_profanity'] = df['body'].apply(\n",
    "        lambda x: int(any(word in str(x).lower() for word in profanity_words))\n",
    "    )\n",
    "    df['has_adword'] = df['body'].apply(\n",
    "        lambda x: int(any(word in str(x).lower() for word in ad_words))\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ”— ê·œì¹™-ëŒ“ê¸€ ìƒí˜¸ì‘ìš© íŠ¹ì§• ìƒì„± ì¤‘...\")\n",
    "    df['rule_body_jaccard'] = [\n",
    "        jaccard_similarity(rule, body) \n",
    "        for rule, body in zip(df['rule'], df['body'])\n",
    "    ]\n",
    "    \n",
    "    print(\"âœ… íŠ¹ì§• ìƒì„± ì™„ë£Œ!\")\n",
    "    return df\n",
    "\n",
    "# ==========================================================\n",
    "# ğŸ¤– Cross-Encoder ì…ë ¥ ì¤€ë¹„ í•¨ìˆ˜\n",
    "def prepare_cross_encoder_input(rule, body, positive_ex1=None, positive_ex2=None, \n",
    "                               negative_ex1=None, negative_ex2=None):\n",
    "    \"\"\"Cross-Encoderë¥¼ ìœ„í•œ ì…ë ¥ í…ìŠ¤íŠ¸ ì¤€ë¹„\"\"\"\n",
    "    rule_text = str(rule).strip()\n",
    "    comment_text = str(body).strip()\n",
    "    \n",
    "    examples_text = \"\"\n",
    "    if pd.notna(positive_ex1) and str(positive_ex1).strip():\n",
    "        examples_text += f\" [ê¸ì •ì˜ˆì‹œ] {str(positive_ex1).strip()}\"\n",
    "    if pd.notna(negative_ex1) and str(negative_ex1).strip():\n",
    "        examples_text += f\" [ë¶€ì •ì˜ˆì‹œ] {str(negative_ex1).strip()}\"\n",
    "    \n",
    "    full_input = f\"[ê·œì¹™] {rule_text}{examples_text} [ëŒ“ê¸€] {comment_text}\"\n",
    "    return full_input\n",
    "\n",
    "# ==========================================================\n",
    "# ğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì§• ìƒì„±\n",
    "print(\"ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...\")\n",
    "\n",
    "# íŠ¹ì§• ìƒì„± ì‹¤í–‰\n",
    "train_df = create_features(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìˆ˜ì¹˜ íŠ¹ì§• ì •ì˜\n",
    "numerical_cols = [\n",
    "    'body_len', 'rule_len', 'body_words', 'url_cnt', 'exc_cnt', 'q_cnt',\n",
    "    'upper_rt', 'rep_run', 'emoji_rt', 'has_profanity', 'has_adword', \n",
    "    'rule_body_jaccard'\n",
    "]\n",
    "\n",
    "# ìˆ˜ì¹˜ íŠ¹ì§• ì •ê·œí™”\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "numerical_features = scaler.fit_transform(train_df[numerical_cols])\n",
    "\n",
    "# Cross-Encoder ì…ë ¥ ë°ì´í„° ì¤€ë¹„\n",
    "print(\"ğŸ”„ Cross-Encoder ì…ë ¥ ë°ì´í„° ì¤€ë¹„ ì¤‘...\")\n",
    "ce_inputs = []\n",
    "labels = []\n",
    "\n",
    "for idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"ì…ë ¥ ë°ì´í„° ì²˜ë¦¬\"):\n",
    "    ce_input = prepare_cross_encoder_input(\n",
    "        row['rule'], \n",
    "        row['body'],\n",
    "        row.get('positive_example_1'),\n",
    "        row.get('positive_example_2'),\n",
    "        row.get('negative_example_1'),\n",
    "        row.get('negative_example_2')\n",
    "    )\n",
    "    ce_inputs.append(ce_input)\n",
    "    labels.append(int(row['rule_violation']))\n",
    "\n",
    "ce_inputs = np.array(ce_inputs)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"âœ… {len(ce_inputs)}ê°œì˜ Cross-Encoder ì…ë ¥ ìŒ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bea179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# ğŸ—ï¸ ìµœì¢… ëª¨ë¸ í›ˆë ¨ (ì „ì²´ ë°ì´í„° ì‚¬ìš©)\n",
    "from sentence_transformers import CrossEncoder, InputExample\n",
    "\n",
    "print(\"ğŸ—ï¸ ì „ì²´ ë°ì´í„°ì…‹ìœ¼ë¡œ ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì‹œì‘!\")\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "output_model_path = './model_output/final_cross_encoder_model'\n",
    "os.makedirs(os.path.dirname(output_model_path), exist_ok=True) # ì €ì¥ í´ë” ìƒì„±\n",
    "\n",
    "# ìµœì¢… ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model_name = 'microsoft/deberta-v3-small'\n",
    "final_model = CrossEncoder(model_name, num_labels=1, device=device)\n",
    "\n",
    "# ì „ì²´ í›ˆë ¨ ë°ì´í„°ë¡œ ì˜ˆì‹œ ìƒì„±\n",
    "print(\"ğŸ“š ì „ì²´ í›ˆë ¨ ì˜ˆì‹œ ìƒì„± ì¤‘...\")\n",
    "train_examples = []\n",
    "\n",
    "for i in tqdm(range(len(ce_inputs)), desc=\"ìµœì¢… í›ˆë ¨ ë°ì´í„° ì²˜ë¦¬\"):\n",
    "    ce_input = ce_inputs[i]\n",
    "    \n",
    "    if '[ëŒ“ê¸€]' in ce_input:\n",
    "        rule_part = ce_input.split('[ëŒ“ê¸€]')[0].strip()\n",
    "        comment_part = ce_input.split('[ëŒ“ê¸€]')[1].strip()\n",
    "    else:\n",
    "        parts = ce_input.split()\n",
    "        mid = len(parts) // 2\n",
    "        rule_part = ' '.join(parts[:mid])\n",
    "        comment_part = ' '.join(parts[mid:])\n",
    "    \n",
    "    train_examples.append(\n",
    "        InputExample(texts=[rule_part, comment_part], label=float(labels[i]))\n",
    "    )\n",
    "\n",
    "# ìµœì¢… í›ˆë ¨ ì„¤ì •\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "warmup_steps = max(1, int(len(train_dataloader) * 0.1))\n",
    "\n",
    "print(f\"ğŸš€ ìµœì¢… ëª¨ë¸ í›ˆë ¨ (ì—í­: 4, ë°°ì¹˜: 16)\")\n",
    "\n",
    "# ìµœì¢… ëª¨ë¸ í›ˆë ¨\n",
    "final_model.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    epochs=4,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=output_model_path,\n",
    "    save_best_model=True,\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# ğŸ’¾ ëª¨ë¸ ë° ì „ì²˜ë¦¬ ê°ì²´ ì €ì¥\n",
    "print(\"ğŸ’¾ ëª¨ë¸ ë° ì „ì²˜ë¦¬ ê°ì²´ ì €ì¥ ì¤‘...\")\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥ ê²½ë¡œ ë³€ìˆ˜ ì‚¬ìš©\n",
    "output_dir = './model_output'\n",
    "\n",
    "# 1. Cross-Encoder ëª¨ë¸ì€ ì´ë¯¸ output_pathì— ì €ì¥ë¨\n",
    "print(f\"âœ… Cross-Encoder ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {os.path.abspath(output_dir)}/final_cross_encoder_model\")\n",
    "\n",
    "# 2. Scaler ì €ì¥\n",
    "joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))\n",
    "print(f\"âœ… Scaler ì €ì¥ ì™„ë£Œ: {os.path.join(output_dir, 'scaler.pkl')}\")\n",
    "\n",
    "# 3. ìˆ˜ì¹˜ íŠ¹ì§• ì»¬ëŸ¼ëª… ì €ì¥\n",
    "with open(os.path.join(output_dir, 'numerical_cols.pkl'), 'wb') as f:\n",
    "    pickle.dump(numerical_cols, f)\n",
    "print(f\"âœ… ìˆ˜ì¹˜ íŠ¹ì§• ì»¬ëŸ¼ëª… ì €ì¥ ì™„ë£Œ: {os.path.join(output_dir, 'numerical_cols.pkl')}\")\n",
    "\n",
    "# 4. íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ í•¨ìˆ˜ë“¤ì„ í¬í•¨í•œ ìœ í‹¸ë¦¬í‹° ì €ì¥\n",
    "feature_utils = {\n",
    "    'count_urls': count_urls,\n",
    "    'count_exclaims': count_exclaims,\n",
    "    'count_questions': count_questions,\n",
    "    'upper_ratio': upper_ratio,\n",
    "    'repeat_char_max': repeat_char_max,\n",
    "    'jaccard_similarity': jaccard_similarity,\n",
    "    'create_features': create_features,\n",
    "    'prepare_cross_encoder_input': prepare_cross_encoder_input\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, 'feature_utils.pkl'), 'wb') as f:\n",
    "    pickle.dump(feature_utils, f)\n",
    "print(f\"âœ… íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ í•¨ìˆ˜ë“¤ ì €ì¥ ì™„ë£Œ: {os.path.join(output_dir, 'feature_utils.pkl')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffad6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# ğŸ“‹ ì €ì¥ëœ íŒŒì¼ë“¤ í™•ì¸\n",
    "output_dir = './model_output'\n",
    "print(f\"\\nğŸ“‹ '{output_dir}' í´ë”ì— ì €ì¥ëœ íŒŒì¼ë“¤:\")\n",
    "for file in os.listdir(output_dir):\n",
    "    file_path = os.path.join(output_dir, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        file_size = os.path.getsize(file_path) / (1024*1024)  # MB\n",
    "        print(f\"  - {file}: {file_size:.2f} MB\")\n",
    "\n",
    "print(f\"\\nğŸ‰ ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“¦ ì´ì œ '{output_dir}' í´ë”ì— ì €ì¥ëœ íŒŒì¼ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ì €ì¥ëœ ëª¨ë¸ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸\n",
    "print(\"\\nğŸ§ª ì €ì¥ëœ ëª¨ë¸ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸...\")\n",
    "test_rule = \"No spam or promotional content\"\n",
    "test_body = \"Check out this amazing deal! Buy now!\"\n",
    "\n",
    "test_input = prepare_cross_encoder_input(test_rule, test_body)\n",
    "if '[ëŒ“ê¸€]' in test_input:\n",
    "    rule_part = test_input.split('[ëŒ“ê¸€]')[0].strip()\n",
    "    comment_part = test_input.split('[ëŒ“ê¸€]')[1].strip()\n",
    "else:\n",
    "    parts = test_input.split()\n",
    "    mid = len(parts) // 2\n",
    "    rule_part = ' '.join(parts[:mid])\n",
    "    comment_part = ' '.join(parts[mid:])\n",
    "\n",
    "# í›ˆë ¨ì´ ëë‚œ final_model ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°”ë¡œ ì˜ˆì¸¡\n",
    "test_pred = final_model.predict([[rule_part, comment_part]])\n",
    "test_prob = torch.sigmoid(torch.tensor(test_pred)).item()\n",
    "\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ê·œì¹™: {test_rule}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ëŒ“ê¸€: {test_body}\")\n",
    "print(f\"ìœ„ë°˜ í™•ë¥ : {test_prob:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c025e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
