{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“ Reddit ê·œì¹™ ìœ„ë°˜ íƒì§€ ì•™ìƒë¸” ëª¨ë¸ ì¶”ë¡ \n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ì‚¬ì „ì— í›ˆë ¨ëœ **Cross-Encoder + LGBM ì•™ìƒë¸” ëª¨ë¸**ì„ ë¶ˆëŸ¬ì™€ `test.csv` ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³ , `submission.csv` íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "### **âœ… ì‹¤í–‰ ì „ í™•ì¸ ì‚¬í•­**\n",
    "1. **ì¸í„°ë„·(Internet) OFF**: ë…¸íŠ¸ë¶ ì„¤ì •ì—ì„œ ì¸í„°ë„·ì´ êº¼ì ¸ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\n",
    "2. **ë°ì´í„°ì…‹ ì¶”ê°€**:\n",
    "    - **ëª¨ë¸ ë°ì´í„°ì…‹**: `model_output` í´ë”ê°€ í¬í•¨ëœ Kaggle ë°ì´í„°ì…‹ (ì˜ˆ: `my-rule-violation-ensemble-model`)\n",
    "    - **ì›ë³¸ ë°ì´í„°ì…‹**: `test.csv`ê°€ í¬í•¨ëœ ëŒ€íšŒ ë°ì´í„°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.sparse import hstack\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ë‹¨ê³„: ëª¨ë¸ ë° ì „ì²˜ë¦¬ ê°ì²´ ë¡œë“œ\n",
    "\n",
    "í›ˆë ¨ ì‹œ ì €ì¥í–ˆë˜ ëª¨ë“  êµ¬ì„± ìš”ì†Œ(Cross-Encoder, LGBM ëª¨ë¸, Scaler, OneHotEncoder, ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸)ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ëª¨ë¸ ë° ê°ì²´ê°€ ì €ì¥ëœ ê²½ë¡œ ì„¤ì • ---\n",
    "# ë³¸ì¸ì˜ Kaggle ë°ì´í„°ì…‹ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •í•´ì£¼ì„¸ìš”.\n",
    "MODEL_PATH = '/kaggle/input/my-rule-violation-ensemble-model/model_output/'\n",
    "\n",
    "print(f\"ëª¨ë¸ ê²½ë¡œ: {MODEL_PATH}\")\n",
    "\n",
    "# 1. Cross-Encoder ëª¨ë¸ ë¡œë“œ\n",
    "cross_encoder_path = os.path.join(MODEL_PATH, 'final_cross_encoder_model')\n",
    "cross_encoder_model = CrossEncoder(cross_encoder_path)\n",
    "print(\"   - Cross-Encoder ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# 2. LGBM ëª¨ë¸ ë¡œë“œ\n",
    "lgbm_model = joblib.load(os.path.join(MODEL_PATH, 'lgbm_model.pkl'))\n",
    "print(\"   - LGBM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# 3. Scaler ë° OneHotEncoder ë¡œë“œ\n",
    "scaler = joblib.load(os.path.join(MODEL_PATH, 'scaler.pkl'))\n",
    "onehot_encoder = joblib.load(os.path.join(MODEL_PATH, 'onehot_encoder.pkl'))\n",
    "print(\"   - Scaler ë° OneHotEncoder ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# 4. ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ\n",
    "with open(os.path.join(MODEL_PATH, 'numerical_cols.pkl'), 'rb') as f:\n",
    "    numerical_cols = pickle.load(f)\n",
    "print(\"   - ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë“  ëª¨ë¸ ë° ì „ì²˜ë¦¬ ê°ì²´ ë¡œë”© ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ ë° íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§\n",
    "\n",
    "`test.csv`ë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , í›ˆë ¨ ê³¼ì •ê³¼ ë™ì¼í•œ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ í•¨ìˆ˜ë“¤ì„ ì •ì˜í•˜ê³  ì ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ í•¨ìˆ˜ë“¤ (í›ˆë ¨ ë…¸íŠ¸ë¶ê³¼ ë™ì¼) ---\n",
    "def count_urls(text):\n",
    "    return len(re.findall(r'https?://\\S+|www\\.\\S+', str(text)))\n",
    "def count_exclaims(text):\n",
    "    return str(text).count('!')\n",
    "def count_questions(text):\n",
    "    return str(text).count('?')\n",
    "def upper_ratio(text):\n",
    "    s = str(text)\n",
    "    letters = [c for c in s if c.isalpha()]\n",
    "    if not letters: return 0.0\n",
    "    return sum(1 for c in letters if c.isupper()) / len(letters)\n",
    "def repeat_char_max(text):\n",
    "    longest = 1\n",
    "    last, cur = '', 0\n",
    "    for ch in str(text):\n",
    "        if ch == last: cur += 1\n",
    "        else: longest, cur, last = max(longest, cur), 1, ch\n",
    "    return max(longest, cur)\n",
    "def jaccard_similarity(text1, text2):\n",
    "    set1 = set(str(text1).lower().split())\n",
    "    set2 = set(str(text2).lower().split())\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['body_len'] = df['body'].astype(str).str.len()\n",
    "    df['rule_len'] = df['rule'].astype(str).str.len()\n",
    "    df['body_words'] = df['body'].astype(str).str.split().str.len()\n",
    "    df['url_cnt'] = df['body'].apply(count_urls)\n",
    "    df['exc_cnt'] = df['body'].apply(count_exclaims)\n",
    "    df['q_cnt'] = df['body'].apply(count_questions)\n",
    "    df['upper_rt'] = df['body'].apply(upper_ratio)\n",
    "    df['rep_run'] = df['body'].apply(repeat_char_max)\n",
    "    df['rule_body_jaccard'] = [jaccard_similarity(r, b) for r, b in zip(df['rule'], df['body'])]\n",
    "    return df\n",
    "\n",
    "def prepare_cross_encoder_input(rule, body, positive_ex1=None, negative_ex1=None):\n",
    "    rule_text, comment_text = str(rule).strip(), str(body).strip()\n",
    "    examples_text = \"\"\n",
    "    if pd.notna(positive_ex1) and str(positive_ex1).strip():\n",
    "        examples_text += f\" [ê¸ì •ì˜ˆì‹œ] {str(positive_ex1).strip()}\"\n",
    "    if pd.notna(negative_ex1) and str(negative_ex1).strip():\n",
    "        examples_text += f\" [ë¶€ì •ì˜ˆì‹œ] {str(negative_ex1).strip()}\"\n",
    "    return f\"[ê·œì¹™] {rule_text}{examples_text} [ëŒ“ê¸€] {comment_text}\"\n",
    "\n",
    "# --- test.csv ë¡œë“œ ë° ì „ì²˜ë¦¬ ì‹¤í–‰ ---\n",
    "test_df = pd.read_csv('/kaggle/input/reddit-rule-violation-prediction/test.csv')\n",
    "print(\"test.csv ë¡œë“œ ì™„ë£Œ.\")\n",
    "\n",
    "test_df_featured = create_features(test_df)\n",
    "print(\"íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ë‹¨ê³„: ì˜ˆì¸¡ ì‹¤í–‰ ë° ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "\n",
    "ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ í™•ë¥ ì„ ê³„ì‚°í•˜ê³  `submission.csv` íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Cross-Encoder ì…ë ¥ ì¤€ë¹„ ë° ì˜ˆì¸¡ ---\n",
    "print(\"1/4: Cross-Encoder ì…ë ¥ ì¤€ë¹„ ì¤‘...\")\n",
    "test_ce_inputs = []\n",
    "for row in tqdm(test_df.itertuples(), total=len(test_df)):\n",
    "    test_ce_inputs.append(prepare_cross_encoder_input(\n",
    "        row.rule, row.body, \n",
    "        getattr(row, 'positive_example_1', None), \n",
    "        getattr(row, 'negative_example_1', None)\n",
    "    ))\n",
    "\n",
    "test_predict_examples = []\n",
    "for ce_input in test_ce_inputs:\n",
    "    if '[ëŒ“ê¸€]' in ce_input:\n",
    "        rule_part, comment_part = ce_input.split('[ëŒ“ê¸€]', 1)\n",
    "        test_predict_examples.append([rule_part.strip(), comment_part.strip()])\n",
    "    else:\n",
    "        test_predict_examples.append([ce_input.strip(), \"\"])\n",
    "\n",
    "print(\"2/4: Cross-Encoderë¡œ semantic score ì˜ˆì¸¡ ì¤‘...\")\n",
    "test_ce_scores = cross_encoder_model.predict(test_predict_examples, show_progress_bar=True, batch_size=64)\n",
    "test_df_featured['ce_score'] = test_ce_scores\n",
    "\n",
    "# --- 2. LGBM ì…ë ¥ ì¤€ë¹„ ---\n",
    "print(\"3/4: LGBM ì…ë ¥ì„ ìœ„í•œ ë°ì´í„° ë³€í™˜ ì¤‘...\")\n",
    "test_numerical_features = scaler.transform(test_df_featured[numerical_cols])\n",
    "test_categorical_features = onehot_encoder.transform(test_df_featured[['subreddit']])\n",
    "X_test_lgbm = hstack([test_numerical_features, test_categorical_features])\n",
    "\n",
    "# --- 3. ìµœì¢… ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„± ---\n",
    "print(\"4/4: ìµœì¢… ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\")\n",
    "final_predictions_proba = lgbm_model.predict_proba(X_test_lgbm)[:, 1]\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'row_id': test_df['row_id'],\n",
    "    'rule_violation': final_predictions_proba\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ! `submission.csv` íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"ìƒì„±ëœ íŒŒì¼ ìƒ˜í”Œ:\")\n",
    "display(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
