{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Reddit Rule Violation - LightGBM \uc131\ub2a5 \ucd5c\uc801\ud654 \ubaa8\ub378\n\n",
                "\uc774 \ub178\ud2b8\ubd81\uc740 EDA\uc5d0\uc11c \uc5bb\uc740 \uc778\uc0ac\uc774\ud2b8\ub97c \ubc14\ud0d5\uc73c\ub85c, \uc131\ub2a5\uc744 \ucd5c\ub300\ud55c \ub04c\uc5b4\uc62c\ub9ac\uae30 \uc704\ud55c LightGBM(LGBM) \ubaa8\ub378\uc744 \uad6c\ud604\ud569\ub2c8\ub2e4.\n\n",
                "**\uc8fc\uc694 \uc804\ub7b5:**\n",
                "1. **\ud53c\ucc98 \uc5d4\uc9c0\ub2c8\uc5b4\ub9c1**: \uae30\uc874 \ud14d\uc2a4\ud2b8 \uc2a4\ud0c0\uc77c \ud53c\ucc98 + `subreddit` \ud0c0\uac9f \uc778\ucf54\ub529\n",
                "2. **\ud14d\uc2a4\ud2b8 \ubca1\ud130\ud654**: TF-IDF (Word + Character n-grams)\n",
                "3. **\ubaa8\ub378**: LightGBM Classifier\n",
                "4. **\uac80\uc99d**: Stratified 5-Fold Cross-Validation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. \ub77c\uc774\ube0c\ub7ec\ub9ac \uc784\ud3ec\ud2b8 \ubc0f \ub370\uc774\ud130 \ub85c\ub4dc"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "import warnings\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.metrics import roc_auc_score\n",
                "import lightgbm as lgb\n",
                "from scipy.sparse import hstack\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# \ub370\uc774\ud130 \ub85c\ub4dc\n",
                "train_df = pd.read_csv('train.csv')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. \ud53c\ucc98 \uc5d4\uc9c0\ub2c8\uc5b4\ub9c1 \ud568\uc218 \uc815\uc758\n\n",
                "EDA\uc5d0\uc11c \uc0ac\uc6a9\ud588\ub358 \ud14d\uc2a4\ud2b8 \uc2a4\ud0c0\uc77c \ud53c\ucc98 \uc0dd\uc131 \ud568\uc218\ub4e4\uc744 \uc815\uc758\ud569\ub2c8\ub2e4."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def add_text_features(df):\n",
                "    df['body_len'] = df['body'].apply(len)\n",
                "    df['url_cnt'] = df['body'].apply(lambda x: len(re.findall(r'http\\S+', x)))\n",
                "    df['exc_cnt'] = df['body'].apply(lambda x: x.count('!'))\n",
                "    df['q_cnt'] = df['body'].apply(lambda x: x.count('?'))\n",
                "    df['upper_rt'] = df['body'].apply(lambda x: len([c for c in x if c.isupper()]) / (len(x) + 1e-6))\n",
                "    return df\n",
                "\n",
                "train_df = add_text_features(train_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. \ubaa8\ub378 \ud559\uc2b5 \ubc0f \uad50\ucc28 \uac80\uc99d\n\n",
                "Stratified K-Fold\ub97c \uc0ac\uc6a9\ud558\uc5ec \uad50\ucc28 \uac80\uc99d\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. \uac01 Fold \ub0b4\ubd80\uc5d0\uc11c **\ud0c0\uac9f \uc778\ucf54\ub529**\uc744 \uc218\ud589\ud558\uc5ec \ub370\uc774\ud130 \ub204\uc124(Leakage)\uc744 \ubc29\uc9c0\ud569\ub2c8\ub2e4."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "target = 'rule_violation'\n",
                "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
                "\n",
                "oof_preds = np.zeros(len(train_df))\n",
                "auc_scores = []\n",
                "\n",
                "# \ud14d\uc2a4\ud2b8 \uc785\ub825 \uc0dd\uc131\n",
                "train_df['text_input'] = train_df['rule'] + \" [SEP] \" + train_df['body']\n",
                "\n",
                "for fold, (train_idx, val_idx) in enumerate(kf.split(train_df, train_df[target])):\n",
                "    print(f\"--- Fold {fold+1} ---\")\n",
                "    \n",
                "    # \ub370\uc774\ud130 \ubd84\ub9ac\n",
                "    X_train, X_val = train_df.iloc[train_idx], train_df.iloc[val_idx]\n",
                "    y_train, y_val = X_train[target], X_val[target]\n",
                "    \n",
                "    # 1. \ud0c0\uac9f \uc778\ucf54\ub529 (\ud3f4\ub4dc \ub0b4\uc5d0\uc11c \uc218\ud589)\n",
                "    subreddit_map = y_train.groupby(X_train['subreddit']).mean()\n",
                "    X_train['subreddit_encoded'] = X_train['subreddit'].map(subreddit_map)\n",
                "    X_val['subreddit_encoded'] = X_val['subreddit'].map(subreddit_map)\n",
                "    X_train['subreddit_encoded'].fillna(y_train.mean(), inplace=True)\n",
                "    X_val['subreddit_encoded'].fillna(y_train.mean(), inplace=True)\n",
                "    \n",
                "    # 2. TF-IDF \ubca1\ud130\ud654\n",
                "    word_vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_features=10000)\n",
                "    char_vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 4), min_df=3, max_features=10000)\n",
                "    \n",
                "    X_train_text_word = word_vectorizer.fit_transform(X_train['text_input'])\n",
                "    X_val_text_word = word_vectorizer.transform(X_val['text_input'])\n",
                "    \n",
                "    X_train_text_char = char_vectorizer.fit_transform(X_train['text_input'])\n",
                "    X_val_text_char = char_vectorizer.transform(X_val['text_input'])\n",
                "    \n",
                "    # 3. \ud53c\ucc98 \uacb0\ud569\n",
                "    numeric_features = ['body_len', 'url_cnt', 'exc_cnt', 'q_cnt', 'upper_rt', 'subreddit_encoded']\n",
                "    X_train_numeric = X_train[numeric_features].values\n",
                "    X_val_numeric = X_val[numeric_features].values\n",
                "    \n",
                "    X_train_combined = hstack([X_train_text_word, X_train_text_char, X_train_numeric]).tocsr()\n",
                "    X_val_combined = hstack([X_val_text_word, X_val_text_char, X_val_numeric]).tocsr()\n",
                "    \n",
                "    # 4. LightGBM \ubaa8\ub378 \ud559\uc2b5\n",
                "    model = lgb.LGBMClassifier(\n",
                "        objective='binary',\n",
                "        metric='auc',\n",
                "        n_estimators=2000,\n",
                "        learning_rate=0.01,\n",
                "        num_leaves=31,\n",
                "        max_depth=-1,\n",
                "        subsample=0.8,\n",
                "        colsample_bytree=0.8, \n",
                "        reg_alpha=0.1,\n",
                "        reg_lambda=0.1,\n",
                "        random_state=42,\n",
                "        n_jobs=-1\n",
                "        # GPU \uc0ac\uc6a9 \uc2dc \uc8fc\uc11d \ud574\uc81c\n",
                "        # device='gpu'\n",
                "    )\n",
                "    \n",
                "    # Early stopping\uc740 callback\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n",
                "    callbacks = [lgb.early_stopping(100, verbose=False), lgb.log_evaluation(100)]\n",
                "    \n",
                "    model.fit(X_train_combined, y_train,\n",
                "              eval_set=[(X_val_combined, y_val)],\n",
                "              eval_metric='auc',\n",
                "              callbacks=callbacks)\n",
                "    \n",
                "    val_preds = model.predict_proba(X_val_combined)[:, 1]\n",
                "    oof_preds[val_idx] = val_preds\n",
                "    auc = roc_auc_score(y_val, val_preds)\n",
                "    auc_scores.append(auc)\n",
                "    print(f\"[Fold {fold+1}] AUC = {auc:.4f}\")\n",
                "\n",
                "print(\"\\n--- \ucd5c\uc885 \uacb0\uacfc ---\")\n",
                "print(f\"CV AUC: {np.mean(auc_scores):.4f} +/- {np.std(auc_scores):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. (\ucc38\uace0) \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \uc608\uce21 \ubc0f \uc81c\ucd9c \ud30c\uc77c \uc0dd\uc131\n\n",
                "\uc2e4\uc81c \ub300\ud68c \uc81c\ucd9c\uc744 \uc704\ud574\uc11c\ub294 \uc804\uccb4 \ud559\uc2b5 \ub370\uc774\ud130\ub85c \ubaa8\ub378\uc744 \uc7ac\ud559\uc2b5\ud558\uace0 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc5d0 \ub300\ud574 \uc608\uce21\ud574\uc57c \ud569\ub2c8\ub2e4."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def make_submission_lgb(train_df, test_df):\n",
                "    print(\"\uc804\uccb4 \ub370\uc774\ud130\ub85c \uc7ac\ud559\uc2b5 \ubc0f \uc608\uce21\uc744 \uc2dc\uc791\ud569\ub2c8\ub2e4...\")\n",
                "    \n",
                "    # \ud53c\ucc98 \uc5d4\uc9c0\ub2c8\uc5b4\ub9c1\n",
                "    train_df = add_text_features(train_df)\n",
                "    test_df = add_text_features(test_df)\n",
                "    \n",
                "    # \ud14d\uc2a4\ud2b8 \uc785\ub825 \uc0dd\uc131\n",
                "    train_df['text_input'] = train_df['rule'] + \" [SEP] \" + train_df['body']\n",
                "    test_df['text_input'] = test_df['rule'] + \" [SEP] \" + test_df['body']\n",
                "    \n",
                "    # \ud0c0\uac9f \uc778\ucf54\ub529 (\uc804\uccb4 \ud559\uc2b5 \ub370\uc774\ud130 \uae30\uc900)\n",
                "    subreddit_map = train_df.groupby('subreddit')[target].mean()\n",
                "    train_df['subreddit_encoded'] = train_df['subreddit'].map(subreddit_map)\n",
                "    test_df['subreddit_encoded'] = test_df['subreddit'].map(subreddit_map)\n",
                "    train_df['subreddit_encoded'].fillna(train_df[target].mean(), inplace=True)\n",
                "    test_df['subreddit_encoded'].fillna(train_df[target].mean(), inplace=True)\n",
                "\n",
                "    # TF-IDF \ubca1\ud130\ud654\n",
                "    word_vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_features=10000)\n",
                "    char_vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 4), min_df=3, max_features=10000)\n",
                "\n",
                "    X_train_text_word = word_vectorizer.fit_transform(train_df['text_input'])\n",
                "    X_test_text_word = word_vectorizer.transform(test_df['text_input'])\n",
                "    \n",
                "    X_train_text_char = char_vectorizer.fit_transform(train_df['text_input'])\n",
                "    X_test_text_char = char_vectorizer.transform(test_df['text_input'])\n",
                "    \n",
                "    # \ud53c\ucc98 \uacb0\ud569\n",
                "    numeric_features = ['body_len', 'url_cnt', 'exc_cnt', 'q_cnt', 'upper_rt', 'subreddit_encoded']\n",
                "    X_train_numeric = train_df[numeric_features].values\n",
                "    X_test_numeric = test_df[numeric_features].values\n",
                "\n",
                "    X_train_combined = hstack([X_train_text_word, X_train_text_char, X_train_numeric]).tocsr()\n",
                "    X_test_combined = hstack([X_test_text_word, X_test_text_char, X_test_numeric]).tocsr()\n",
                "    \n",
                "    # LightGBM \ubaa8\ub378 \ud559\uc2b5\n",
                "    # \ucd5c\uc801 n_estimators\ub294 \uad50\ucc28 \uac80\uc99d\uc5d0\uc11c \ub098\uc628 \ud3c9\uade0\uc801\uc778 best_iteration_\uc744 \ucc38\uace0\ud558\uc5ec \uc124\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n",
                "    model = lgb.LGBMClassifier(\n",
                "        objective='binary',\n",
                "        metric='auc',\n",
                "        n_estimators=500, # \uc608\uc2dc \uac12\n",
                "        learning_rate=0.01,\n",
                "        num_leaves=31,\n",
                "        max_depth=-1,\n",
                "        subsample=0.8,\n",
                "        colsample_bytree=0.8,\n",
                "        reg_alpha=0.1,\n",
                "        reg_lambda=0.1,\n",
                "        random_state=42,\n",
                "        n_jobs=-1\n",
                "    )\n",
                "    \n",
                "    model.fit(X_train_combined, train_df[target])\n",
                "    \n",
                "    # \uc608\uce21 \ubc0f \uc81c\ucd9c \ud30c\uc77c \uc0dd\uc131\n",
                "    predictions = model.predict_proba(X_test_combined)[:, 1]\n",
                "    submission_df = pd.DataFrame({'row_id': test_df['row_id'], 'rule_violation': predictions})\n",
                "    submission_df.to_csv('submission_lgb.csv', index=False)\n",
                "    print(\"\\n\uc81c\ucd9c \ud30c\uc77c 'submission_lgb.csv'\uac00 \uc0dd\uc131\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\")\n",
                "    return submission_df\n",
                "\n",
                "# test.csv \ud30c\uc77c\uc774 \uc788\ub2e4\uba74 \uc544\ub798 \ucf54\ub4dc\uc758 \uc8fc\uc11d\uc744 \ud574\uc81c\ud558\uc5ec \uc2e4\ud589\ud558\uc138\uc694.\n",
                "# test_df = pd.read_csv('test.csv')\n",
                "# submission = make_submission_lgb(train_df.copy(), test_df.copy()) \n",
                "# print(submission.head())\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}