{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77bb5054",
   "metadata": {},
   "source": [
    "# 📝 Reddit 규칙 위반 탐지 모델 추론\n",
    "\n",
    "이 노트북은 사전에 훈련된 모델을 불러와 `test.csv` 데이터에 대한 예측을 수행하고, `submission.csv` 파일을 생성합니다.\n",
    "\n",
    "### **✅ 실행 전 확인 사항**\n",
    "1. **인터넷(Internet) OFF**: 노트북 설정에서 인터넷이 꺼져있는지 확인하세요.\n",
    "2. **데이터셋 추가**:\n",
    "    - **모델 데이터셋**: `juhwancom/reddit-violation-model-v1`\n",
    "    - **원본 데이터셋**: `test.csv`가 포함된 대회 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354c3ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "print(\"✅ 라이브러리 임포트 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276d2bb1",
   "metadata": {},
   "source": [
    "## 1단계: 모델 및 전처리 도구 불러오기\n",
    "\n",
    "캐글 데이터셋으로 업로드한 모델과 전처리에 사용했던 `scaler`, 함수 등을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecef4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📂 --- 경로 설정 ---\n",
    "# ❗❗ 'your-original-competition-data' 부분은 test.csv가 있는 실제 데이터셋 이름으로 바꿔주세요.\n",
    "MODEL_DATASET_PATH = '/kaggle/input/reddit-violation-model-v1/model_output/'\n",
    "TEST_CSV_PATH = '/kaggle/input/your-original-competition-data/test.csv'\n",
    "\n",
    "# 모델 및 전처리 도구 경로\n",
    "MODEL_PATH = os.path.join(MODEL_DATASET_PATH, 'final_cross_encoder_model')\n",
    "SCALER_PATH = os.path.join(MODEL_DATASET_PATH, 'scaler.pkl')\n",
    "COLS_PATH = os.path.join(MODEL_DATASET_PATH, 'numerical_cols.pkl')\n",
    "UTILS_PATH = os.path.join(MODEL_DATASET_PATH, 'feature_utils.pkl')\n",
    "\n",
    "# 🧠 --- 모델 및 도구 불러오기 ---\n",
    "print(f\"모델을 로컬 경로에서 불러옵니다: {MODEL_PATH}\")\n",
    "final_model = CrossEncoder(MODEL_PATH)\n",
    "print(\"모델 로딩 완료.\")\n",
    "\n",
    "# 전처리 도구들 불러오기\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "numerical_cols = pickle.load(open(COLS_PATH, 'rb'))\n",
    "feature_utils = pickle.load(open(UTILS_PATH, 'rb'))\n",
    "\n",
    "# 저장했던 함수들을 현재 노트북에서 사용할 수 있도록 변수에 할당\n",
    "create_features = feature_utils['create_features']\n",
    "prepare_cross_encoder_input = feature_utils['prepare_cross_encoder_input']\n",
    "print(\"전처리 도구 로딩 완료.\")\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 디바이스: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4a8079",
   "metadata": {},
   "source": [
    "## 2단계: 테스트 데이터 준비\n",
    "불러온 함수들을 사용하여 `test.csv` 데이터를 모델이 예측할 수 있는 형태로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d49910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 로드\n",
    "test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "\n",
    "# 1. 학습 때와 동일한 특징 생성\n",
    "print(\"테스트 데이터 특징 생성 중...\")\n",
    "test_features_df = create_features(test_df)\n",
    "\n",
    "# 2. 학습 때 사용했던 스케일러로 수치 특징 변환\n",
    "print(\"수치 특징 스케일링 중...\")\n",
    "numerical_features_test = scaler.transform(test_features_df[numerical_cols])\n",
    "\n",
    "# 3. CrossEncoder 모델 입력 형식으로 변환\n",
    "print(\"모델 입력 데이터 준비 중...\")\n",
    "test_inputs = []\n",
    "for idx, row in tqdm(test_features_df.iterrows(), total=len(test_features_df)):\n",
    "    ce_input = prepare_cross_encoder_input(\n",
    "        row['rule'], \n",
    "        row['body']\n",
    "        # 테스트 데이터에는 예시(example) 컬럼이 없으므로 생략\n",
    "    )\n",
    "    \n",
    "    if '[댓글]' in ce_input:\n",
    "        rule_part = ce_input.split('[댓글]')[0].strip()\n",
    "        comment_part = ce_input.split('[댓글]')[1].strip()\n",
    "        test_inputs.append([rule_part, comment_part])\n",
    "    else:\n",
    "        parts = ce_input.split()\n",
    "        mid = len(parts) // 2\n",
    "        rule_part = ' '.join(parts[:mid])\n",
    "        comment_part = ' '.join(parts[mid:])\n",
    "        test_inputs.append([rule_part, comment_part])\n",
    "        \n",
    "print(\"데이터 준비 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de76b82",
   "metadata": {},
   "source": [
    "## 3단계: 예측 실행 및 제출 파일 생성\n",
    "준비된 데이터를 모델에 입력하여 예측을 수행하고, 결과를 `submission.csv` 파일로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c68cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 --- 예측 실행 ---\n",
    "print(\"모델 예측 시작...\")\n",
    "# GPU 메모리 상황에 따라 batch_size를 조절할 수 있습니다. (예: 16, 32, 64)\n",
    "predictions = final_model.predict(test_inputs, show_progress_bar=True, batch_size=32)\n",
    "\n",
    "# CrossEncoder의 출력은 로짓(logit)이므로, 확률로 변환하기 위해 시그모이드 함수 적용\n",
    "probabilities = 1 / (1 + np.exp(-predictions))\n",
    "print(\"예측 완료.\")\n",
    "\n",
    "# 📄 --- 제출 파일 생성 ---\n",
    "submission_df = pd.DataFrame({\n",
    "    'row_id': test_df['row_id'],\n",
    "    'rule_violation': probabilities\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\n✅ submission.csv 파일 생성이 완료되었습니다!\")\n",
    "print(\"제출 파일 샘플:\")\n",
    "display(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
