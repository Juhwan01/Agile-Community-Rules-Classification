{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Reddit Rule Violation - XGBoost 성능 최적화 모델\n",
                "\n",
                "이 노트북은 EDA에서 얻은 인사이트를 바탕으로, 성능을 최대한 끌어올리기 위한 XGBoost 모델을 구현합니다.\n",
                "\n",
                "**주요 전략:**\n",
                "1. **피처 엔지니어링**: 기존 텍스트 스타일 피처 + `subreddit` 타겟 인코딩\n",
                "2. **텍스트 벡터화**: TF-IDF (Word + Character n-grams)\n",
                "3. **모델**: XGBoost Classifier\n",
                "4. **검증**: Stratified 5-Fold Cross-Validation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. 라이브러리 임포트 및 데이터 로드"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "import warnings\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.metrics import roc_auc_score\n",
                "import xgboost as xgb\n",
                "from scipy.sparse import hstack\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# 데이터 로드\n",
                "train_df = pd.read_csv('train.csv')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. 피처 엔지니어링 함수 정의\n",
                "\n",
                "EDA에서 사용했던 텍스트 스타일 피처 생성 함수들을 정의합니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "def add_text_features(df):\n",
                "    df['body_len'] = df['body'].apply(len)\n",
                "    df['url_cnt'] = df['body'].apply(lambda x: len(re.findall(r'http\\S+', x)))\n",
                "    df['exc_cnt'] = df['body'].apply(lambda x: x.count('!'))\n",
                "    df['q_cnt'] = df['body'].apply(lambda x: x.count('?'))\n",
                "    df['upper_rt'] = df['body'].apply(lambda x: len([c for c in x if c.isupper()]) / (len(x) + 1e-6))\n",
                "    return df\n",
                "\n",
                "train_df = add_text_features(train_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. 모델 학습 및 교차 검증\n",
                "\n",
                "Stratified K-Fold를 사용하여 교차 검증을 수행합니다. 각 Fold 내부에서 **타겟 인코딩**을 수행하여 데이터 누설(Leakage)을 방지합니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Fold 1 ---\n",
                        "[0]\tvalidation_0-auc:0.72676\n",
                        "[100]\tvalidation_0-auc:0.83174\n",
                        "[166]\tvalidation_0-auc:0.83450\n",
                        "[Fold 1] AUC = 0.8353\n",
                        "--- Fold 2 ---\n",
                        "[0]\tvalidation_0-auc:0.61144\n",
                        "[100]\tvalidation_0-auc:0.77483\n",
                        "[200]\tvalidation_0-auc:0.78701\n",
                        "[300]\tvalidation_0-auc:0.79005\n",
                        "[308]\tvalidation_0-auc:0.79012\n",
                        "[Fold 2] AUC = 0.7913\n",
                        "--- Fold 3 ---\n",
                        "[0]\tvalidation_0-auc:0.68424\n",
                        "[100]\tvalidation_0-auc:0.82222\n",
                        "[200]\tvalidation_0-auc:0.83171\n",
                        "[300]\tvalidation_0-auc:0.83824\n",
                        "[358]\tvalidation_0-auc:0.83732\n",
                        "[Fold 3] AUC = 0.8390\n",
                        "--- Fold 4 ---\n",
                        "[0]\tvalidation_0-auc:0.64249\n",
                        "[100]\tvalidation_0-auc:0.78982\n",
                        "[200]\tvalidation_0-auc:0.80116\n",
                        "[300]\tvalidation_0-auc:0.80150\n",
                        "[318]\tvalidation_0-auc:0.80179\n",
                        "[Fold 4] AUC = 0.8057\n",
                        "--- Fold 5 ---\n",
                        "[0]\tvalidation_0-auc:0.68920\n",
                        "[100]\tvalidation_0-auc:0.81500\n",
                        "[200]\tvalidation_0-auc:0.82302\n",
                        "[300]\tvalidation_0-auc:0.82618\n",
                        "[327]\tvalidation_0-auc:0.82635\n",
                        "[Fold 5] AUC = 0.8287\n",
                        "\n",
                        "--- 최종 결과 ---\n",
                        "CV AUC: 0.8200 +/- 0.0184\n"
                    ]
                }
            ],
            "source": [
                "target = 'rule_violation'\n",
                "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
                "\n",
                "oof_preds = np.zeros(len(train_df))\n",
                "auc_scores = []\n",
                "\n",
                "# 텍스트 입력 생성\n",
                "train_df['text_input'] = train_df['rule'] + \" [SEP] \" + train_df['body']\n",
                "\n",
                "for fold, (train_idx, val_idx) in enumerate(kf.split(train_df, train_df[target])):\n",
                "    print(f\"--- Fold {fold+1} ---\")\n",
                "    \n",
                "    # 데이터 분리\n",
                "    X_train, X_val = train_df.iloc[train_idx], train_df.iloc[val_idx]\n",
                "    y_train, y_val = X_train[target], X_val[target]\n",
                "    \n",
                "    # 1. 타겟 인코딩 (폴드 내에서 수행)\n",
                "    subreddit_map = y_train.groupby(X_train['subreddit']).mean()\n",
                "    X_train['subreddit_encoded'] = X_train['subreddit'].map(subreddit_map)\n",
                "    X_val['subreddit_encoded'] = X_val['subreddit'].map(subreddit_map)\n",
                "    X_train['subreddit_encoded'].fillna(y_train.mean(), inplace=True)\n",
                "    X_val['subreddit_encoded'].fillna(y_train.mean(), inplace=True)\n",
                "    \n",
                "    # 2. TF-IDF 벡터화\n",
                "    word_vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_features=10000)\n",
                "    char_vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 4), min_df=3, max_features=10000)\n",
                "    \n",
                "    X_train_text_word = word_vectorizer.fit_transform(X_train['text_input'])\n",
                "    X_val_text_word = word_vectorizer.transform(X_val['text_input'])\n",
                "    \n",
                "    X_train_text_char = char_vectorizer.fit_transform(X_train['text_input'])\n",
                "    X_val_text_char = char_vectorizer.transform(X_val['text_input'])\n",
                "    \n",
                "    # 3. 피처 결합\n",
                "    numeric_features = ['body_len', 'url_cnt', 'exc_cnt', 'q_cnt', 'upper_rt', 'subreddit_encoded']\n",
                "    X_train_numeric = X_train[numeric_features].values\n",
                "    X_val_numeric = X_val[numeric_features].values\n",
                "    \n",
                "    X_train_combined = hstack([X_train_text_word, X_train_text_char, X_train_numeric])\n",
                "    X_val_combined = hstack([X_val_text_word, X_val_text_char, X_val_numeric])\n",
                "    \n",
                "    # 4. XGBoost 모델 학습\n",
                "    model = xgb.XGBClassifier(\n",
                "        objective='binary:logistic',\n",
                "        eval_metric='auc',\n",
                "        n_estimators=1000,\n",
                "        learning_rate=0.05,\n",
                "        max_depth=6,\n",
                "        subsample=0.7,\n",
                "        colsample_bytree=0.7,\n",
                "        use_label_encoder=False,\n",
                "        random_state=42,\n",
                "        early_stopping_rounds=50, #<- 위치 이동\n",
                "        # GPU 사용 시 주석 해제\n",
                "        tree_method='gpu_hist' \n",
                "    )\n",
                "    \n",
                "    model.fit(X_train_combined, y_train,\n",
                "              eval_set=[(X_val_combined, y_val)],\n",
                "              verbose=100)\n",
                "    \n",
                "    val_preds = model.predict_proba(X_val_combined)[:, 1]\n",
                "    oof_preds[val_idx] = val_preds\n",
                "    auc = roc_auc_score(y_val, val_preds)\n",
                "    auc_scores.append(auc)\n",
                "    print(f\"[Fold {fold+1}] AUC = {auc:.4f}\")\n",
                "\n",
                "print(\"\\n--- 최종 결과 ---\")\n",
                "print(f\"CV AUC: {np.mean(auc_scores):.4f} +/- {np.std(auc_scores):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. (참고) 테스트 데이터 예측 및 제출 파일 생성\n",
                "\n",
                "실제 대회 제출을 위해서는 전체 학습 데이터로 모델을 재학습하고 테스트 데이터에 대해 예측해야 합니다. 아래는 그 과정을 담은 예시 코드입니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def make_submission(train_df, test_df):\n",
                "    print(\"전체 데이터로 재학습 및 예측을 시작합니다...\")\n",
                "    \n",
                "    # 피처 엔지니어링\n",
                "    train_df = add_text_features(train_df)\n",
                "    test_df = add_text_features(test_df)\n",
                "    \n",
                "    # 텍스트 입력 생성\n",
                "    train_df['text_input'] = train_df['rule'] + \" [SEP] \" + train_df['body']\n",
                "    test_df['text_input'] = test_df['rule'] + \" [SEP] \" + test_df['body']\n",
                "    \n",
                "    # 타겟 인코딩 (전체 학습 데이터 기준)\n",
                "    subreddit_map = train_df.groupby('subreddit')[target].mean()\n",
                "    train_df['subreddit_encoded'] = train_df['subreddit'].map(subreddit_map)\n",
                "    test_df['subreddit_encoded'] = test_df['subreddit'].map(subreddit_map)\n",
                "    train_df['subreddit_encoded'].fillna(train_df[target].mean(), inplace=True)\n",
                "    test_df['subreddit_encoded'].fillna(train_df[target].mean(), inplace=True)\n",
                "\n",
                "    # TF-IDF 벡터화\n",
                "    word_vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_features=10000)\n",
                "    char_vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 4), min_df=3, max_features=10000)\n",
                "\n",
                "    X_train_text_word = word_vectorizer.fit_transform(train_df['text_input'])\n",
                "    X_test_text_word = word_vectorizer.transform(test_df['text_input'])\n",
                "    \n",
                "    X_train_text_char = char_vectorizer.fit_transform(train_df['text_input'])\n",
                "    X_test_text_char = char_vectorizer.transform(test_df['text_input'])\n",
                "    \n",
                "    # 피처 결합\n",
                "    numeric_features = ['body_len', 'url_cnt', 'exc_cnt', 'q_cnt', 'upper_rt', 'subreddit_encoded']\n",
                "    X_train_numeric = train_df[numeric_features].values\n",
                "    X_test_numeric = test_df[numeric_features].values\n",
                "\n",
                "    X_train_combined = hstack([X_train_text_word, X_train_text_char, X_train_numeric])\n",
                "    X_test_combined = hstack([X_test_text_word, X_test_text_char, X_test_numeric])\n",
                "    \n",
                "    # XGBoost 모델 학습\n",
                "    model = xgb.XGBClassifier(\n",
                "        objective='binary:logistic',\n",
                "        eval_metric='auc',\n",
                "        n_estimators=1200, # early stopping 최적값 근사치로 설정\n",
                "        learning_rate=0.05,\n",
                "        max_depth=6,\n",
                "        subsample=0.7,\n",
                "        colsample_bytree=0.7,\n",
                "        use_label_encoder=False,\n",
                "        random_state=42\n",
                "    )\n",
                "    \n",
                "    model.fit(X_train_combined, train_df[target])\n",
                "    \n",
                "    # 예측 및 제출 파일 생성\n",
                "    predictions = model.predict_proba(X_test_combined)[:, 1]\n",
                "    submission_df = pd.DataFrame({'row_id': test_df['row_id'], 'rule_violation': predictions})\n",
                "    submission_df.to_csv('submission.csv', index=False)\n",
                "    print(\"\\n제출 파일 'submission.csv'가 생성되었습니다.\")\n",
                "    return submission_df\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "전체 데이터로 재학습 및 예측을 시작합니다...\n"
                    ]
                },
                {
                    "ename": "ValueError",
                    "evalue": "Must have at least 1 validation dataset for early stopping.",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# test.csv 파일이 있다면 아래 코드의 주석을 해제하여 실행하세요.\u001b[39;00m\n\u001b[32m      2\u001b[39m test_df = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mtest.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m submission = \u001b[43mmake_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(submission.head())\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mmake_submission\u001b[39m\u001b[34m(train_df, test_df)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# XGBoost 모델 학습\u001b[39;00m\n\u001b[32m     38\u001b[39m model = xgb.XGBClassifier(\n\u001b[32m     39\u001b[39m     objective=\u001b[33m'\u001b[39m\u001b[33mbinary:logistic\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     40\u001b[39m     eval_metric=\u001b[33m'\u001b[39m\u001b[33mauc\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m     early_stopping_rounds=\u001b[32m100\u001b[39m   \u001b[38;5;66;03m# 조기 종료 인내심 증가\u001b[39;00m\n\u001b[32m     51\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# 예측 및 제출 파일 생성\u001b[39;00m\n\u001b[32m     56\u001b[39m predictions = model.predict_proba(X_test_combined)[:, \u001b[32m1\u001b[39m]\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Anaconda3\\envs\\daycon\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Anaconda3\\envs\\daycon\\Lib\\site-packages\\xgboost\\sklearn.py:1682\u001b[39m, in \u001b[36mXGBClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1660\u001b[39m model, metric, params, feature_weights = \u001b[38;5;28mself\u001b[39m._configure_fit(\n\u001b[32m   1661\u001b[39m     xgb_model, params, feature_weights\n\u001b[32m   1662\u001b[39m )\n\u001b[32m   1663\u001b[39m train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[32m   1664\u001b[39m     missing=\u001b[38;5;28mself\u001b[39m.missing,\n\u001b[32m   1665\u001b[39m     X=X,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1679\u001b[39m     feature_types=\u001b[38;5;28mself\u001b[39m.feature_types,\n\u001b[32m   1680\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1685\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1696\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.objective):\n\u001b[32m   1697\u001b[39m     \u001b[38;5;28mself\u001b[39m.objective = params[\u001b[33m\"\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m\"\u001b[39m]\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Anaconda3\\envs\\daycon\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Anaconda3\\envs\\daycon\\Lib\\site-packages\\xgboost\\training.py:184\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    182\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    183\u001b[39m     bst.update(dtrain, iteration=i, fobj=obj)\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb_container\u001b[49m\u001b[43m.\u001b[49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    185\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    187\u001b[39m bst = cb_container.after_training(bst)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Anaconda3\\envs\\daycon\\Lib\\site-packages\\xgboost\\callback.py:267\u001b[39m, in \u001b[36mCallbackContainer.after_iteration\u001b[39m\u001b[34m(self, model, epoch, dtrain, evals)\u001b[39m\n\u001b[32m    265\u001b[39m     metric_score = _parse_eval_str(score)\n\u001b[32m    266\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_history(metric_score, epoch)\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m ret = \u001b[38;5;28many\u001b[39m(c.after_iteration(model, epoch, \u001b[38;5;28mself\u001b[39m.history) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks)\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Anaconda3\\envs\\daycon\\Lib\\site-packages\\xgboost\\callback.py:267\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    265\u001b[39m     metric_score = _parse_eval_str(score)\n\u001b[32m    266\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_history(metric_score, epoch)\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m ret = \u001b[38;5;28many\u001b[39m(\u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks)\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Anaconda3\\envs\\daycon\\Lib\\site-packages\\xgboost\\callback.py:463\u001b[39m, in \u001b[36mEarlyStopping.after_iteration\u001b[39m\u001b[34m(self, model, epoch, evals_log)\u001b[39m\n\u001b[32m    461\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mMust have at least 1 validation dataset for early stopping.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(evals_log.keys()) < \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    465\u001b[39m \u001b[38;5;66;03m# Get data name\u001b[39;00m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data:\n",
                        "\u001b[31mValueError\u001b[39m: Must have at least 1 validation dataset for early stopping."
                    ]
                }
            ],
            "source": [
                "# test.csv 파일이 있다면 아래 코드의 주석을 해제하여 실행하세요.\n",
                "test_df = pd.read_csv('test.csv')\n",
                "submission = make_submission(train_df.copy(), test_df.copy()) \n",
                "print(submission.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "daycon",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
