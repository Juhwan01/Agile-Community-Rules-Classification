{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75351324",
   "metadata": {},
   "source": [
    "##  Cross-Encoder + LightGBM Ensemble Model\n",
    "\n",
    "This notebook builds a powerful ensemble model to determine if a comment violates a community rule.\n",
    "\n",
    "### **Methodology**\n",
    "1.  **Feature Engineering**: Creates numerical and categorical features based on EDA insights.\n",
    "2.  **Cross-Encoder Training**: A `deberta-v3-small` model is fine-tuned to understand the semantic relationship between a rule and a comment, generating a 'semantic score'.\n",
    "3.  **LightGBM Training**: An LGBM model is trained on a combination of the engineered features and the semantic score from the Cross-Encoder.\n",
    "4.  **Ensemble Pipeline**: The final model uses this two-stage process for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📚 필수 라이브러리 설치 및 임포트\n",
    "!pip install sentence-transformers lightgbm scikit-learn pandas numpy torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "print(f\"GPU 사용 가능: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 디바이스: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e6f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 📊 데이터 로드 및 기본 정보 확인\n",
    "# ==========================================================\n",
    "# 로컬 경로에서 학습 데이터를 불러옵니다.\n",
    "# If you don't have 'train.csv', create a dummy file to run the notebook\n",
    "if not os.path.exists('train.csv'):\n",
    "    dummy_data = {\n",
    "        'body': ['This is a great post!', 'Check out my website www.spam.com', 'I disagree with this rule.', 'legal advice is not allowed here', 'where can i watch the game?', 'no advertising please'],\n",
    "        'rule': ['Be nice', 'No spam', 'Follow the rules', 'No legal advice', 'No illegal content', 'No Advertising'],\n",
    "        'subreddit': ['hearthstone', 'soccerstreams', 'legaladvice', 'legaladvice', 'soccerstreams', 'sex'],\n",
    "        'rule_violation': [0, 1, 0, 1, 1, 1],\n",
    "        'positive_example_1': [np.nan, 'our product is the best', np.nan, 'asking for a lawyer is legal advice', 'youtube.com/stream', 'dont promote your onlyfans'],\n",
    "        'negative_example_1': ['thanks for sharing', np.nan, 'I love this sub', 'I am not a lawyer but...', 'what time is the match?', 'i have a question about my body']\n",
    "    }\n",
    "    train_df = pd.DataFrame(dummy_data)\n",
    "    train_df.to_csv('train.csv', index=False)\n",
    "    print(\"Dummy 'train.csv' created.\")\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "print(f\"🔍 데이터 형태: {train_df.shape}\")\n",
    "print(f\"🎯 타겟 분포: {train_df['rule_violation'].value_counts().to_dict()}\")\n",
    "print(\"\\n📋 데이터 샘플:\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66819f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 🛠️ 특징 엔지니어링 함수 정의\n",
    "# ==========================================================\n",
    "def count_urls(text):\n",
    "    return len(re.findall(r'https?://\\S+|www\\.\\S+', str(text)))\n",
    "\n",
    "def count_exclaims(text):\n",
    "    return str(text).count('!')\n",
    "\n",
    "def count_questions(text):\n",
    "    return str(text).count('?')\n",
    "\n",
    "def upper_ratio(text):\n",
    "    s = str(text)\n",
    "    letters = [c for c in s if c.isalpha()]\n",
    "    if not letters:\n",
    "        return 0.0\n",
    "    upp = sum(1 for c in letters if c.isupper())\n",
    "    return upp / len(letters)\n",
    "\n",
    "def repeat_char_max(text):\n",
    "    longest = 1\n",
    "    last = ''\n",
    "    cur = 0\n",
    "    for ch in str(text):\n",
    "        if ch == last:\n",
    "            cur += 1\n",
    "        else:\n",
    "            longest = max(longest, cur)\n",
    "            cur = 1\n",
    "            last = ch\n",
    "    longest = max(longest, cur)\n",
    "    return longest\n",
    "\n",
    "def jaccard_similarity(text1, text2):\n",
    "    set1 = set(str(text1).lower().split())\n",
    "    set2 = set(str(text2).lower().split())\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    print(\"📏 기본 텍스트 특징 생성 중...\")\n",
    "    df['body_len'] = df['body'].astype(str).str.len()\n",
    "    df['rule_len'] = df['rule'].astype(str).str.len()\n",
    "    df['body_words'] = df['body'].astype(str).str.split().str.len()\n",
    "    print(\"🎨 스타일 특징 생성 중...\")\n",
    "    df['url_cnt'] = df['body'].apply(count_urls)\n",
    "    df['exc_cnt'] = df['body'].apply(count_exclaims)\n",
    "    df['q_cnt'] = df['body'].apply(count_questions)\n",
    "    df['upper_rt'] = df['body'].apply(upper_ratio)\n",
    "    df['rep_run'] = df['body'].apply(repeat_char_max)\n",
    "    print(\"🔗 규칙-댓글 상호작용 특징 생성 중...\")\n",
    "    df['rule_body_jaccard'] = [jaccard_similarity(rule, body) for rule, body in zip(df['rule'], df['body'])]\n",
    "    print(\"✅ 특징 생성 완료!\")\n",
    "    return df\n",
    "\n",
    "def prepare_cross_encoder_input(rule, body, positive_ex1=None, negative_ex1=None):\n",
    "    rule_text = str(rule).strip()\n",
    "    comment_text = str(body).strip()\n",
    "    examples_text = \"\"\n",
    "    if pd.notna(positive_ex1) and str(positive_ex1).strip():\n",
    "        examples_text += f\" [긍정예시] {str(positive_ex1).strip()}\"\n",
    "    if pd.notna(negative_ex1) and str(negative_ex1).strip():\n",
    "        examples_text += f\" [부정예시] {str(negative_ex1).strip()}\"\n",
    "    full_input = f\"[규칙] {rule_text}{examples_text} [댓글] {comment_text}\"\n",
    "    return full_input\n",
    "\n",
    "# ==========================================================\n",
    "# 📊 데이터 전처리 및 특징 생성 실행\n",
    "# ==========================================================\n",
    "print(\"🔧 데이터 전처리 시작...\")\n",
    "train_df_featured = create_features(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 🤖 Cross-Encoder 입력 준비\n",
    "# ==========================================================\n",
    "print(\"🔄 Cross-Encoder 입력 데이터 준비 중...\")\n",
    "ce_inputs = []\n",
    "labels = []\n",
    "for idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"CE 입력 데이터 처리\"):\n",
    "    ce_input = prepare_cross_encoder_input(\n",
    "        row['rule'], row['body'],\n",
    "        row.get('positive_example_1'),\n",
    "        row.get('negative_example_1')\n",
    "    )\n",
    "    ce_inputs.append(ce_input)\n",
    "    labels.append(int(row['rule_violation']))\n",
    "\n",
    "ce_inputs = np.array(ce_inputs)\n",
    "labels = np.array(labels)\n",
    "print(f\"✅ {len(ce_inputs)}개의 Cross-Encoder 입력 쌍 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bea179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 🏗️ 1단계: Cross-Encoder 모델 훈련\n",
    "# ==========================================================\n",
    "from sentence_transformers import CrossEncoder, InputExample\n",
    "\n",
    "print(\"🏗️ 전체 데이터셋으로 Cross-Encoder 모델 훈련 시작!\")\n",
    "output_model_path = './model_output/final_cross_encoder_model'\n",
    "os.makedirs(output_model_path, exist_ok=True)\n",
    "\n",
    "model_name = 'microsoft/deberta-v3-small'\n",
    "cross_encoder_model = CrossEncoder(model_name, num_labels=1, device=device)\n",
    "\n",
    "print(\"📚 전체 훈련 예시 생성 중...\")\n",
    "train_examples = []\n",
    "for i in tqdm(range(len(ce_inputs)), desc=\"최종 훈련 데이터 처리\"):\n",
    "    ce_input = ce_inputs[i]\n",
    "    if '[댓글]' in ce_input:\n",
    "        rule_part, comment_part = ce_input.split('[댓글]', 1)\n",
    "    else: # Fallback if separator not found\n",
    "        rule_part, comment_part = ce_input, \"\"\n",
    "    train_examples.append(\n",
    "        InputExample(texts=[rule_part.strip(), comment_part.strip()], label=float(labels[i]))\n",
    "    )\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "warmup_steps = max(1, int(len(train_dataloader) * 0.1))\n",
    "\n",
    "print(f\"🚀 최종 모델 훈련 (에폭: 4, 배치: 16)\")\n",
    "cross_encoder_model.fit(\n",
    "    train_dataloader=train_dataloader, epochs=4, warmup_steps=warmup_steps,\n",
    "    output_path=output_model_path, save_best_model=True, show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e1894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✨ NEW: =======================================================\n",
    "# 🤖 2단계 준비: Cross-Encoder로 Semantic Feature 생성\n",
    "# ==========================================================\n",
    "print(\"🔮 Cross-Encoder를 사용하여 semantic score 예측 중...\")\n",
    "\n",
    "# 훈련된 모델을 사용하여 예측을 수행하기 위한 입력 형식으로 변환\n",
    "predict_examples = []\n",
    "for i in tqdm(range(len(ce_inputs)), desc=\"예측용 데이터 변환\"):\n",
    "    ce_input = ce_inputs[i]\n",
    "    if '[댓글]' in ce_input:\n",
    "        rule_part, comment_part = ce_input.split('[댓글]', 1)\n",
    "    else:\n",
    "        rule_part, comment_part = ce_input, \"\"\n",
    "    predict_examples.append([rule_part.strip(), comment_part.strip()])\n",
    "\n",
    "# 예측 수행 (raw logit scores)\n",
    "ce_predictions = cross_encoder_model.predict(predict_examples, show_progress_bar=True)\n",
    "\n",
    "# 예측 점수를 DataFrame의 새로운 컬럼으로 추가\n",
    "train_df_featured['ce_score'] = ce_predictions\n",
    "print(\"✅ Semantic score ('ce_score')가 특징에 추가되었습니다.\")\n",
    "display(train_df_featured[['body', 'rule', 'ce_score', 'rule_violation']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ded25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✨ NEW: =======================================================\n",
    "# 🛠️ 2단계 준비: LGBM을 위한 데이터 준비\n",
    "# ==========================================================\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "print(\"🔧 LGBM 모델을 위한 특징 스케일링 및 인코딩 중...\")\n",
    "\n",
    "# 1. 수치 특징 (Numerical Features)\n",
    "numerical_cols = [\n",
    "    'body_len', 'rule_len', 'body_words', 'url_cnt', 'exc_cnt', 'q_cnt',\n",
    "    'upper_rt', 'rep_run', 'rule_body_jaccard', \n",
    "    'ce_score' # Cross-Encoder 예측 점수 포함!\n",
    "]\n",
    "scaler = StandardScaler()\n",
    "numerical_features = scaler.fit_transform(train_df_featured[numerical_cols])\n",
    "print(f\"🔢 {len(numerical_cols)}개의 수치 특징 스케일링 완료.\")\n",
    "\n",
    "# 2. 범주형 특징 (Categorical Features) - EDA 인사이트 반영!\n",
    "categorical_cols = ['subreddit']\n",
    "onehot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "categorical_features = onehot_encoder.fit_transform(train_df_featured[categorical_cols])\n",
    "print(f\"📋 {len(categorical_cols)}개의 범주형 특징 원-핫 인코딩 완료.\")\n",
    "\n",
    "# 3. 모든 특징 결합\n",
    "X_lgbm = hstack([numerical_features, categorical_features])\n",
    "y_lgbm = train_df_featured['rule_violation'].values\n",
    "\n",
    "print(f\"✅ LGBM 훈련 데이터 준비 완료. 최종 형태: {X_lgbm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c780c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✨ NEW: =======================================================\n",
    "# 🏗️ 2단계: LightGBM 모델 훈련\n",
    "# ==========================================================\n",
    "import lightgbm as lgb\n",
    "\n",
    "print(\"🚀 LightGBM 모델 훈련 시작...\")\n",
    "\n",
    "lgbm_model = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    metric='auc',\n",
    "    n_estimators=1000, # 조기 종료를 사용하므로 넉넉하게 설정\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.8\n",
    ")\n",
    "\n",
    "# LGBM 훈련\n",
    "lgbm_model.fit(X_lgbm, y_lgbm, \n",
    "             eval_set=[(X_lgbm, y_lgbm)],\n",
    "             eval_metric='auc',\n",
    "             callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "\n",
    "print(\"✅ LightGBM 모델 훈련 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 💾 최종 모델 및 전처리 객체 저장\n",
    "# ==========================================================\n",
    "print(\"💾 모델 및 전처리 객체 저장 중...\")\n",
    "output_dir = './model_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Cross-Encoder 모델은 이미 output_path에 저장됨\n",
    "print(f\"✅ Cross-Encoder 모델 저장 완료: {os.path.abspath(output_model_path)}\")\n",
    "\n",
    "# ✨ NEW: 2. LGBM 모델 저장\n",
    "joblib.dump(lgbm_model, os.path.join(output_dir, 'lgbm_model.pkl'))\n",
    "print(f\"✅ LGBM 모델 저장 완료: {os.path.join(output_dir, 'lgbm_model.pkl')}\")\n",
    "\n",
    "# 3. Scaler 저장\n",
    "joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))\n",
    "print(f\"✅ Scaler 저장 완료: {os.path.join(output_dir, 'scaler.pkl')}\")\n",
    "\n",
    "# ✨ NEW: 4. OneHotEncoder 저장\n",
    "joblib.dump(onehot_encoder, os.path.join(output_dir, 'onehot_encoder.pkl'))\n",
    "print(f\"✅ OneHotEncoder 저장 완료: {os.path.join(output_dir, 'onehot_encoder.pkl')}\")\n",
    "\n",
    "# 5. 수치 특징 컬럼명 저장\n",
    "with open(os.path.join(output_dir, 'numerical_cols.pkl'), 'wb') as f:\n",
    "    pickle.dump(numerical_cols, f)\n",
    "print(f\"✅ 수치 특징 컬럼명 저장 완료: {os.path.join(output_dir, 'numerical_cols.pkl')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffad6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 🧪 저장된 전체 파이프라인 테스트\n",
    "# ==========================================================\n",
    "print(\"\\n🧪 저장된 앙상블 모델 파이프라인 빠른 테스트...\")\n",
    "\n",
    "# 0. 테스트 데이터 준비\n",
    "test_data = {\n",
    "    'body': [\"Check out this amazing deal! Buy now!\", \"I am not a lawyer, but you should probably sue them.\"],\n",
    "    'rule': [\"No spam or promotional content\", \"No giving legal advice\"],\n",
    "    'subreddit': ['soccerstreams', 'legaladvice'],\n",
    "    'positive_example_1': [np.nan, 'you must contact a real lawyer'],\n",
    "    'negative_example_1': [np.nan, 'I recommend seeking professional help']\n",
    "}\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# 1. 특징 엔지니어링\n",
    "test_df_featured = create_features(test_df)\n",
    "\n",
    "# 2. Cross-Encoder로 semantic score 예측\n",
    "test_ce_inputs = []\n",
    "for _, row in test_df.iterrows():\n",
    "    test_ce_inputs.append(prepare_cross_encoder_input(\n",
    "        row['rule'], row['body'], row.get('positive_example_1'), row.get('negative_example_1')\n",
    "    ))\n",
    "\n",
    "test_predict_examples = []\n",
    "for ce_input in test_ce_inputs:\n",
    "    rule_part, comment_part = ce_input.split('[댓글]', 1)\n",
    "    test_predict_examples.append([rule_part.strip(), comment_part.strip()])\n",
    "\n",
    "test_ce_scores = cross_encoder_model.predict(test_predict_examples)\n",
    "test_df_featured['ce_score'] = test_ce_scores\n",
    "\n",
    "# 3. LGBM을 위한 데이터 변환\n",
    "test_numerical_features = scaler.transform(test_df_featured[numerical_cols])\n",
    "test_categorical_features = onehot_encoder.transform(test_df_featured[categorical_cols])\n",
    "X_test_lgbm = hstack([test_numerical_features, test_categorical_features])\n",
    "\n",
    "# 4. 최종 예측 (LGBM)\n",
    "final_predictions_proba = lgbm_model.predict_proba(X_test_lgbm)[:, 1]\n",
    "\n",
    "# 결과 출력\n",
    "for i, proba in enumerate(final_predictions_proba):\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"테스트 케이스 {i+1}:\")\n",
    "    print(f\"  - 커뮤니티: {test_df.iloc[i]['subreddit']}\")\n",
    "    print(f\"  - 규칙: {test_df.iloc[i]['rule']}\")\n",
    "    print(f\"  - 댓글: {test_df.iloc[i]['body']}\")\n",
    "    print(f\"  - 🔥 위반 확률: {proba:.4f}\")\n",
    "\n",
    "print(\"\\n✅ 모든 작업 완료!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
