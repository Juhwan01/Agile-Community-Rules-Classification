{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75351324",
   "metadata": {},
   "source": [
    "##  Cross-Encoder + LightGBM Ensemble Model\n",
    "\n",
    "This notebook builds a powerful ensemble model to determine if a comment violates a community rule.\n",
    "\n",
    "### **Methodology**\n",
    "1.  **Feature Engineering**: Creates numerical and categorical features based on EDA insights.\n",
    "2.  **Cross-Encoder Training**: A `deberta-v3-small` model is fine-tuned to understand the semantic relationship between a rule and a comment, generating a 'semantic score'.\n",
    "3.  **LightGBM Training**: An LGBM model is trained on a combination of the engineered features and the semantic score from the Cross-Encoder.\n",
    "4.  **Ensemble Pipeline**: The final model uses this two-stage process for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸\n",
    "!pip install sentence-transformers lightgbm scikit-learn pandas numpy torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
    "print(f\"GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e6f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# ğŸ“Š ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì •ë³´ í™•ì¸\n",
    "# ==========================================================\n",
    "# ë¡œì»¬ ê²½ë¡œì—ì„œ í•™ìŠµ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "# If you don't have 'train.csv', create a dummy file to run the notebook\n",
    "if not os.path.exists('train.csv'):\n",
    "    dummy_data = {\n",
    "        'body': ['This is a great post!', 'Check out my website www.spam.com', 'I disagree with this rule.', 'legal advice is not allowed here', 'where can i watch the game?', 'no advertising please'],\n",
    "        'rule': ['Be nice', 'No spam', 'Follow the rules', 'No legal advice', 'No illegal content', 'No Advertising'],\n",
    "        'subreddit': ['hearthstone', 'soccerstreams', 'legaladvice', 'legaladvice', 'soccerstreams', 'sex'],\n",
    "        'rule_violation': [0, 1, 0, 1, 1, 1],\n",
    "        'positive_example_1': [np.nan, 'our product is the best', np.nan, 'asking for a lawyer is legal advice', 'youtube.com/stream', 'dont promote your onlyfans'],\n",
    "        'negative_example_1': ['thanks for sharing', np.nan, 'I love this sub', 'I am not a lawyer but...', 'what time is the match?', 'i have a question about my body']\n",
    "    }\n",
    "    train_df = pd.DataFrame(dummy_data)\n",
    "    train_df.to_csv('train.csv', index=False)\n",
    "    print(\"Dummy 'train.csv' created.\")\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "print(f\"ğŸ” ë°ì´í„° í˜•íƒœ: {train_df.shape}\")\n",
    "print(f\"ğŸ¯ íƒ€ê²Ÿ ë¶„í¬: {train_df['rule_violation'].value_counts().to_dict()}\")\n",
    "print(\"\\nğŸ“‹ ë°ì´í„° ìƒ˜í”Œ:\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66819f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# ğŸ› ï¸ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ í•¨ìˆ˜ ì •ì˜\n",
    "# ==========================================================\n",
    "def count_urls(text):\n",
    "    return len(re.findall(r'https?://\\S+|www\\.\\S+', str(text)))\n",
    "\n",
    "def count_exclaims(text):\n",
    "    return str(text).count('!')\n",
    "\n",
    "def count_questions(text):\n",
    "    return str(text).count('?')\n",
    "\n",
    "def upper_ratio(text):\n",
    "    s = str(text)\n",
    "    letters = [c for c in s if c.isalpha()]\n",
    "    if not letters:\n",
    "        return 0.0\n",
    "    upp = sum(1 for c in letters if c.isupper())\n",
    "    return upp / len(letters)\n",
    "\n",
    "def repeat_char_max(text):\n",
    "    longest = 1\n",
    "    last = ''\n",
    "    cur = 0\n",
    "    for ch in str(text):\n",
    "        if ch == last:\n",
    "            cur += 1\n",
    "        else:\n",
    "            longest = max(longest, cur)\n",
    "            cur = 1\n",
    "            last = ch\n",
    "    longest = max(longest, cur)\n",
    "    return longest\n",
    "\n",
    "def jaccard_similarity(text1, text2):\n",
    "    set1 = set(str(text1).lower().split())\n",
    "    set2 = set(str(text2).lower().split())\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    print(\"ğŸ“ ê¸°ë³¸ í…ìŠ¤íŠ¸ íŠ¹ì§• ìƒì„± ì¤‘...\")\n",
    "    df['body_len'] = df['body'].astype(str).str.len()\n",
    "    df['rule_len'] = df['rule'].astype(str).str.len()\n",
    "    df['body_words'] = df['body'].astype(str).str.split().str.len()\n",
    "    print(\"ğŸ¨ ìŠ¤íƒ€ì¼ íŠ¹ì§• ìƒì„± ì¤‘...\")\n",
    "    df['url_cnt'] = df['body'].apply(count_urls)\n",
    "    df['exc_cnt'] = df['body'].apply(count_exclaims)\n",
    "    df['q_cnt'] = df['body'].apply(count_questions)\n",
    "    df['upper_rt'] = df['body'].apply(upper_ratio)\n",
    "    df['rep_run'] = df['body'].apply(repeat_char_max)\n",
    "    print(\"ğŸ”— ê·œì¹™-ëŒ“ê¸€ ìƒí˜¸ì‘ìš© íŠ¹ì§• ìƒì„± ì¤‘...\")\n",
    "    df['rule_body_jaccard'] = [jaccard_similarity(rule, body) for rule, body in zip(df['rule'], df['body'])]\n",
    "    print(\"âœ… íŠ¹ì§• ìƒì„± ì™„ë£Œ!\")\n",
    "    return df\n",
    "\n",
    "def prepare_cross_encoder_input(rule, body, positive_ex1=None, negative_ex1=None):\n",
    "    rule_text = str(rule).strip()\n",
    "    comment_text = str(body).strip()\n",
    "    examples_text = \"\"\n",
    "    if pd.notna(positive_ex1) and str(positive_ex1).strip():\n",
    "        examples_text += f\" [ê¸ì •ì˜ˆì‹œ] {str(positive_ex1).strip()}\"\n",
    "    if pd.notna(negative_ex1) and str(negative_ex1).strip():\n",
    "        examples_text += f\" [ë¶€ì •ì˜ˆì‹œ] {str(negative_ex1).strip()}\"\n",
    "    full_input = f\"[ê·œì¹™] {rule_text}{examples_text} [ëŒ“ê¸€] {comment_text}\"\n",
    "    return full_input\n",
    "\n",
    "# ==========================================================\n",
    "# ğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì§• ìƒì„± ì‹¤í–‰\n",
    "# ==========================================================\n",
    "print(\"ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...\")\n",
    "train_df_featured = create_features(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# ğŸ¤– Cross-Encoder ì…ë ¥ ì¤€ë¹„\n",
    "# ==========================================================\n",
    "print(\"ğŸ”„ Cross-Encoder ì…ë ¥ ë°ì´í„° ì¤€ë¹„ ì¤‘...\")\n",
    "ce_inputs = []\n",
    "labels = []\n",
    "for idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"CE ì…ë ¥ ë°ì´í„° ì²˜ë¦¬\"):\n",
    "    ce_input = prepare_cross_encoder_input(\n",
    "        row['rule'], row['body'],\n",
    "        row.get('positive_example_1'),\n",
    "        row.get('negative_example_1')\n",
    "    )\n",
    "    ce_inputs.append(ce_input)\n",
    "    labels.append(int(row['rule_violation']))\n",
    "\n",
    "ce_inputs = np.array(ce_inputs)\n",
    "labels = np.array(labels)\n",
    "print(f\"âœ… {len(ce_inputs)}ê°œì˜ Cross-Encoder ì…ë ¥ ìŒ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bea179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# ğŸ—ï¸ 1ë‹¨ê³„: Cross-Encoder ëª¨ë¸ í›ˆë ¨\n",
    "# ==========================================================\n",
    "from sentence_transformers import CrossEncoder, InputExample\n",
    "\n",
    "print(\"ğŸ—ï¸ ì „ì²´ ë°ì´í„°ì…‹ìœ¼ë¡œ Cross-Encoder ëª¨ë¸ í›ˆë ¨ ì‹œì‘!\")\n",
    "output_model_path = './model_output/final_cross_encoder_model'\n",
    "os.makedirs(output_model_path, exist_ok=True)\n",
    "\n",
    "model_name = 'microsoft/deberta-v3-small'\n",
    "cross_encoder_model = CrossEncoder(model_name, num_labels=1, device=device)\n",
    "\n",
    "print(\"ğŸ“š ì „ì²´ í›ˆë ¨ ì˜ˆì‹œ ìƒì„± ì¤‘...\")\n",
    "train_examples = []\n",
    "for i in tqdm(range(len(ce_inputs)), desc=\"ìµœì¢… í›ˆë ¨ ë°ì´í„° ì²˜ë¦¬\"):\n",
    "    ce_input = ce_inputs[i]\n",
    "    if '[ëŒ“ê¸€]' in ce_input:\n",
    "        rule_part, comment_part = ce_input.split('[ëŒ“ê¸€]', 1)\n",
    "    else: # Fallback if separator not found\n",
    "        rule_part, comment_part = ce_input, \"\"\n",
    "    train_examples.append(\n",
    "        InputExample(texts=[rule_part.strip(), comment_part.strip()], label=float(labels[i]))\n",
    "    )\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "warmup_steps = max(1, int(len(train_dataloader) * 0.1))\n",
    "\n",
    "print(f\"ğŸš€ ìµœì¢… ëª¨ë¸ í›ˆë ¨ (ì—í­: 4, ë°°ì¹˜: 16)\")\n",
    "cross_encoder_model.fit(\n",
    "    train_dataloader=train_dataloader, epochs=4, warmup_steps=warmup_steps,\n",
    "    output_path=output_model_path, save_best_model=True, show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e1894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ¨ NEW: =======================================================\n",
    "# ğŸ¤– 2ë‹¨ê³„ ì¤€ë¹„: Cross-Encoderë¡œ Semantic Feature ìƒì„±\n",
    "# ==========================================================\n",
    "print(\"ğŸ”® Cross-Encoderë¥¼ ì‚¬ìš©í•˜ì—¬ semantic score ì˜ˆì¸¡ ì¤‘...\")\n",
    "\n",
    "# í›ˆë ¨ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "predict_examples = []\n",
    "for i in tqdm(range(len(ce_inputs)), desc=\"ì˜ˆì¸¡ìš© ë°ì´í„° ë³€í™˜\"):\n",
    "    ce_input = ce_inputs[i]\n",
    "    if '[ëŒ“ê¸€]' in ce_input:\n",
    "        rule_part, comment_part = ce_input.split('[ëŒ“ê¸€]', 1)\n",
    "    else:\n",
    "        rule_part, comment_part = ce_input, \"\"\n",
    "    predict_examples.append([rule_part.strip(), comment_part.strip()])\n",
    "\n",
    "# ì˜ˆì¸¡ ìˆ˜í–‰ (raw logit scores)\n",
    "ce_predictions = cross_encoder_model.predict(predict_examples, show_progress_bar=True)\n",
    "\n",
    "# ì˜ˆì¸¡ ì ìˆ˜ë¥¼ DataFrameì˜ ìƒˆë¡œìš´ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€\n",
    "train_df_featured['ce_score'] = ce_predictions\n",
    "print(\"âœ… Semantic score ('ce_score')ê°€ íŠ¹ì§•ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "display(train_df_featured[['body', 'rule', 'ce_score', 'rule_violation']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ded25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ¨ NEW: =======================================================\n",
    "# ğŸ› ï¸ 2ë‹¨ê³„ ì¤€ë¹„: LGBMì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„\n",
    "# ==========================================================\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "print(\"ğŸ”§ LGBM ëª¨ë¸ì„ ìœ„í•œ íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ ë° ì¸ì½”ë”© ì¤‘...\")\n",
    "\n",
    "# 1. ìˆ˜ì¹˜ íŠ¹ì§• (Numerical Features)\n",
    "numerical_cols = [\n",
    "    'body_len', 'rule_len', 'body_words', 'url_cnt', 'exc_cnt', 'q_cnt',\n",
    "    'upper_rt', 'rep_run', 'rule_body_jaccard', \n",
    "    'ce_score' # Cross-Encoder ì˜ˆì¸¡ ì ìˆ˜ í¬í•¨!\n",
    "]\n",
    "scaler = StandardScaler()\n",
    "numerical_features = scaler.fit_transform(train_df_featured[numerical_cols])\n",
    "print(f\"ğŸ”¢ {len(numerical_cols)}ê°œì˜ ìˆ˜ì¹˜ íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ.\")\n",
    "\n",
    "# 2. ë²”ì£¼í˜• íŠ¹ì§• (Categorical Features) - EDA ì¸ì‚¬ì´íŠ¸ ë°˜ì˜!\n",
    "categorical_cols = ['subreddit']\n",
    "onehot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "categorical_features = onehot_encoder.fit_transform(train_df_featured[categorical_cols])\n",
    "print(f\"ğŸ“‹ {len(categorical_cols)}ê°œì˜ ë²”ì£¼í˜• íŠ¹ì§• ì›-í•« ì¸ì½”ë”© ì™„ë£Œ.\")\n",
    "\n",
    "# 3. ëª¨ë“  íŠ¹ì§• ê²°í•©\n",
    "X_lgbm = hstack([numerical_features, categorical_features])\n",
    "y_lgbm = train_df_featured['rule_violation'].values\n",
    "\n",
    "print(f\"âœ… LGBM í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ. ìµœì¢… í˜•íƒœ: {X_lgbm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c780c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ¨ NEW: =======================================================\n",
    "# ğŸ—ï¸ 2ë‹¨ê³„: LightGBM ëª¨ë¸ í›ˆë ¨\n",
    "# ==========================================================\n",
    "import lightgbm as lgb\n",
    "\n",
    "print(\"ğŸš€ LightGBM ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
    "\n",
    "lgbm_model = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    metric='auc',\n",
    "    n_estimators=1000, # ì¡°ê¸° ì¢…ë£Œë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë„‰ë„‰í•˜ê²Œ ì„¤ì •\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.8\n",
    ")\n",
    "\n",
    "# LGBM í›ˆë ¨\n",
    "lgbm_model.fit(X_lgbm, y_lgbm, \n",
    "             eval_set=[(X_lgbm, y_lgbm)],\n",
    "             eval_metric='auc',\n",
    "             callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "\n",
    "print(\"âœ… LightGBM ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# ğŸ’¾ ìµœì¢… ëª¨ë¸ ë° ì „ì²˜ë¦¬ ê°ì²´ ì €ì¥\n",
    "# ==========================================================\n",
    "print(\"ğŸ’¾ ëª¨ë¸ ë° ì „ì²˜ë¦¬ ê°ì²´ ì €ì¥ ì¤‘...\")\n",
    "output_dir = './model_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Cross-Encoder ëª¨ë¸ì€ ì´ë¯¸ output_pathì— ì €ì¥ë¨\n",
    "print(f\"âœ… Cross-Encoder ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {os.path.abspath(output_model_path)}\")\n",
    "\n",
    "# âœ¨ NEW: 2. LGBM ëª¨ë¸ ì €ì¥\n",
    "joblib.dump(lgbm_model, os.path.join(output_dir, 'lgbm_model.pkl'))\n",
    "print(f\"âœ… LGBM ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {os.path.join(output_dir, 'lgbm_model.pkl')}\")\n",
    "\n",
    "# 3. Scaler ì €ì¥\n",
    "joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))\n",
    "print(f\"âœ… Scaler ì €ì¥ ì™„ë£Œ: {os.path.join(output_dir, 'scaler.pkl')}\")\n",
    "\n",
    "# âœ¨ NEW: 4. OneHotEncoder ì €ì¥\n",
    "joblib.dump(onehot_encoder, os.path.join(output_dir, 'onehot_encoder.pkl'))\n",
    "print(f\"âœ… OneHotEncoder ì €ì¥ ì™„ë£Œ: {os.path.join(output_dir, 'onehot_encoder.pkl')}\")\n",
    "\n",
    "# 5. ìˆ˜ì¹˜ íŠ¹ì§• ì»¬ëŸ¼ëª… ì €ì¥\n",
    "with open(os.path.join(output_dir, 'numerical_cols.pkl'), 'wb') as f:\n",
    "    pickle.dump(numerical_cols, f)\n",
    "print(f\"âœ… ìˆ˜ì¹˜ íŠ¹ì§• ì»¬ëŸ¼ëª… ì €ì¥ ì™„ë£Œ: {os.path.join(output_dir, 'numerical_cols.pkl')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffad6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# ğŸ§ª ì €ì¥ëœ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸\n",
    "# ==========================================================\n",
    "print(\"\\nğŸ§ª ì €ì¥ëœ ì•™ìƒë¸” ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸...\")\n",
    "\n",
    "# 0. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„\n",
    "test_data = {\n",
    "    'body': [\"Check out this amazing deal! Buy now!\", \"I am not a lawyer, but you should probably sue them.\"],\n",
    "    'rule': [\"No spam or promotional content\", \"No giving legal advice\"],\n",
    "    'subreddit': ['soccerstreams', 'legaladvice'],\n",
    "    'positive_example_1': [np.nan, 'you must contact a real lawyer'],\n",
    "    'negative_example_1': [np.nan, 'I recommend seeking professional help']\n",
    "}\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# 1. íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§\n",
    "test_df_featured = create_features(test_df)\n",
    "\n",
    "# 2. Cross-Encoderë¡œ semantic score ì˜ˆì¸¡\n",
    "test_ce_inputs = []\n",
    "for _, row in test_df.iterrows():\n",
    "    test_ce_inputs.append(prepare_cross_encoder_input(\n",
    "        row['rule'], row['body'], row.get('positive_example_1'), row.get('negative_example_1')\n",
    "    ))\n",
    "\n",
    "test_predict_examples = []\n",
    "for ce_input in test_ce_inputs:\n",
    "    rule_part, comment_part = ce_input.split('[ëŒ“ê¸€]', 1)\n",
    "    test_predict_examples.append([rule_part.strip(), comment_part.strip()])\n",
    "\n",
    "test_ce_scores = cross_encoder_model.predict(test_predict_examples)\n",
    "test_df_featured['ce_score'] = test_ce_scores\n",
    "\n",
    "# 3. LGBMì„ ìœ„í•œ ë°ì´í„° ë³€í™˜\n",
    "test_numerical_features = scaler.transform(test_df_featured[numerical_cols])\n",
    "test_categorical_features = onehot_encoder.transform(test_df_featured[categorical_cols])\n",
    "X_test_lgbm = hstack([test_numerical_features, test_categorical_features])\n",
    "\n",
    "# 4. ìµœì¢… ì˜ˆì¸¡ (LGBM)\n",
    "final_predictions_proba = lgbm_model.predict_proba(X_test_lgbm)[:, 1]\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "for i, proba in enumerate(final_predictions_proba):\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i+1}:\")\n",
    "    print(f\"  - ì»¤ë®¤ë‹ˆí‹°: {test_df.iloc[i]['subreddit']}\")\n",
    "    print(f\"  - ê·œì¹™: {test_df.iloc[i]['rule']}\")\n",
    "    print(f\"  - ëŒ“ê¸€: {test_df.iloc[i]['body']}\")\n",
    "    print(f\"  - ğŸ”¥ ìœ„ë°˜ í™•ë¥ : {proba:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
